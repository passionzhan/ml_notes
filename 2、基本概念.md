# 基本概念

- 机器学习：致力于研究通过计算的手段，利用经验（数据）来改善系统自身的性能。

- 假设空间(hypothesis space)：又称版本空间(version space)学习目标的可选范围集合。学习任务就是在此范围集合中搜寻到最优假设。

- 图灵测试:

- 经验误差:训练误差

- 泛化误差：

- 模型参数：

  - 超参数(hyper-parameters）：模型训练之前需要人为设置的参数，可以通过交叉验证选择模型的最优超参数。
  - 参数(可学习参数)：模型根据数据可以自动学习出的变量。

- 模型评估方法

  - 留出法（hold-out）：直接将数据集$D$划分为互斥的集合，一个用于训练称训练集(training  set)，一个用于测试，称测试机(test set)。

  - 交叉验证法(cross validation)：将数据集划分为$k$个大小相似的互斥子集，每次用$k-1$个子集用于训练，余下的那个用于测试集。

  - 自助法（bootstrapping）：对包含$m$个样本的数据集采用自助采样(bootstrap sampling) ，采样方式为有放回采样，整个采样过程重复$m$次，得到训练数据集，剩余没被采集到的样本作为测试集。这样$m$次采样中始终不被采到的概率为$(1-\frac{1}{m})^m$。
    $$
    \lim\limits_{m\to\infty}(1-\frac{1}{m})^m = \frac{1}{e}\approx 0.368
    $$
    

<table align=center>
    <tr>
        <th rowspan="2" align=center>真实结果</th>
        <th colspan="2" >预测结果</th>
    </tr>
    <tr>
        <th>正例</th>
        <th>反例</th> 
    </tr>
    <tr>
        <td align=center>正例</td>
		<td>TP(真正例)</td>
		<td>FN(假反例)</td>
    </tr>
    <tr>
        <td align=center>反例</td>
		<td>FP(假正例)</td>
		<td>TN(真正例)</td>
	</tr>
</table>
- 查全率(recall):$R= \frac{TP}{TP+FN}$

- 查准率(precision):$P = \frac{TP}{TP+FP}$

- F1:$F1= \frac{2PR}{P+R}$

  **注意**：$F1$是基于查准率与查全率的调和平均定义的，即$\frac{1}{F1} = \frac{1}{2}(\frac{1}{P} + \frac{1}{R})$

- ROC曲线（Receiver Operating Characteristic）：受试者工作特征曲线。

  若一个学习器A的ROC曲线完全在另一个ROC曲线B的上方，则可以称学习器A性能全完优于学习器B。若两曲线有交叉，则较为合理的判据为ROC曲线下方的面积（即AUC, Area Under Roc Curve）来判断优劣.

- AUC曲线：ROC曲线下方的面积（即AUC, Area Under Roc Curve）

- 偏差(bias)：度量了模型期望预测与真实结果的偏离程度，刻画描述了模型本身对数据的拟合能力；

- 方差(variance)：方差度量了训练集的变化导致学习性能的变化，描述了数据扰动造成的影响；

- 噪声: 表达了当前任务上任何模型所能达到的期望泛化误差的下界，**刻画了学习问题本身的难度**。

- 泛化误差：模型在未见数据上的误差率。泛化误差 = 偏差 + 方差 + 噪声

- 欠拟合：模型过于简单，在训练数据上误差较人类水平差距很大，模型偏差较大。增加模型复杂度，如增加特征数、选用更加复杂模型、多方法集成等。

- 过拟合：模型过于复杂，训练数据上效果较好，测试数据上误差较大，模型方差比较大。解决方法降低模型复杂度，正则化、增加训练数据。

  ![学习模型偏差、方差关系图](.\images\bais variance relation.png"学习模型偏差方差理解")![高偏差](.\images\high bias.png"高偏差")![高方差](.\images\high variance.png"高方差")

- 各种损失函数：
  
  - 0-1损失函数(0-1 loss function)：
    $$
    L(Y,f(X)) = \left \{
    \begin{align}
    1&,&Y\ne f(X)  \\\\
    0&,&Y= f(X)
    \end{align}
    \right.
    $$
    由于0-1损失函数不可导，数学性质不好，因此出现了很多代理损失函数(surrogate loss)来近似0-1损失函数。
  
    记$Y = \{-1,1\},Z = Yf(X)$,
  
  - hinge损失函数：
    $$
    L(Z) = \max (0,1-z)
    $$
  
  - 指数损失函数：
    $$
    L(Z)  = \exp(-Z)
    $$
  
  - 逻辑斯蒂损失(logistic loss)
    $$
    L(Z) = \log (1 + exp(-Z))
    $$
  
    - 二分类交叉熵损失：
  
      标签采用$Y = \{0,1\},Z = Yf(X)$,线性输出为$f(X)$, 非线性输出为$\hat Y =g(X)=\frac{1}{1+\exp^{-f(X)}}$,其含义为预测样本标签为1的概率，即p(y=1|x)=g(x)。
      $$
      L(X) = -\log \left(Y\log \hat Y + (1-Y)\log (1-\hat Y)\right)
      $$
  
  **注意**： 逻辑斯蒂损失是可以看做是标准的对数损失的特殊情况。此时$P(Y|X)  = \frac{1}{1+ \exp(-Yf(X))} = \frac{1}{1+ \exp(-Z)} $
  
  ![损失函数](.\images\损失函数.png"常见损失函数图像")
  
  - 平方损失函数(quadratic loss function)：
    $$
    L(Y,f(X)) = (Y - f(X))^2
    $$
  
  - 绝对损失函数(absolute loss function)：
    $$
    L(Y,f(X)) = |Y - f(X)|
    $$
  
  - 对数损失函数(logarithmic loss function)：
    $$
    L(Y,P(Y|X)) = -\log P(Y|X) 
    $$
  
- 风险函数：损失函数度量模型一次性预测的好坏，风险函数度量平均意义下模型预测的好坏，也即损失函数的期望：
  $$
  R_\mathrm{exp}(f) = \mathrm E_p[L(Y,f(X))]=\int_{\cal X\times \cal Y} L(Y,f(X))P(x,y)\mathrm dx \mathrm dy
  $$

  - 经验风险：上述风险函数是模型关于联合分布P(X,Y)的期望损失，但是P(X,Y)未知，难以计算。经验风险是模型关于训练数据集的平均损失：
    $$
    R_\mathrm{emp}(f) = \frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))
    $$
    根据大数定律，当样本量趋于无穷时，上述经验风险将趋向于期望风险。因此实中，经常利用经验风险来估计期望风险。但另一方面，实际问题中训练样本数有限，甚至很小，利用经验风险来估计期望风险并不理想，因此需要进行一定的矫正。经验风险最小化（Empirical risk minimization, ERM）

    **注意**：极大似然估计就是经验风险最小化的例子，当模型是条件概率分布时，损失函数是对数损失函数时，经验风险最小化就等于极大似然估计。

  - 结构化风险：当样本容量很小，经验风险最小化学习容易产生过拟合现象。结构风险最小化(Structure Risk Minimization)就是为了防止过拟合和提出的风险控制策略。结构风险最小化等价于正则化(Regularization)。结构风险定义如下：
    $$
    R_\mathrm{srm}(f) = \frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i)) + \lambda J(f)
    $$
    其中$J(f)$描述了模型的复杂度。

    **注意**：贝叶斯估计中的最大后验概率估计(Maximum Posterior Probability Estimation, MAP)就是结构风险最小化的例子，当模型是条件概率分布，损失函数时对数损失函数，模型复杂度由模型的先验概率表示，结构风险最小化就等价于最大后验概率估计。最大支持向量机也是一种结构风险最小化的学习模型。

- 正则化(Regularization)：正则化是结构风险最小化策略的实现。通过在经验风险基础上调价惩罚项或者正则化项来实现。正则化项一般是模型复杂度的单调增函数。

- 生成模型(Generative Model)：此类模型先学习数据的联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$。
  $$
  P(Y|X) = \frac{P(X,Y)}{P(X)}
  $$
  典型的生成模型有：朴素贝叶斯法和隐马尔可夫模型。

- 判别模型(Discriminative Model)：判别模型直接通过数据学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测模型。

  典型判别模型有：$k$近邻、支持向量机、最大熵模型、逻辑斯蒂回归等

  - 生成模型可以直接还原建模联合概率分布、学习收敛速度快、样本容量增加时，学习到的模型能更快收敛于真是模型，当存在隐变量时，仍然适用。

  - 判别模型直接学习决策函数或者条件概率，直面预测问题、学习准确率高，可以对数据进行各种抽象、自定义特征等，能简化学习问题。