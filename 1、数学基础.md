# 数学基础
***
常见记号
向量$x = {[x_1,x_2,\cdots,x_n]}^{\mathrm{T}} ​$
矩阵 $A = [a_1,a_2,\cdots,a_n] = (a_{ij}) ​$，其中$a_i(i=1,\cdots,n）​$为列向量

## 1. 矩阵迹
**定义**：$n​$阶方阵$A​$主对角线上元素之和，即$\mathrm{tr}(A)=\sum _{i=1}^na_{ii}​$。
$$
\begin{align}
\mathrm{tr}(A^{\mathrm T})&= \mathrm{tr}(A)  \\\\
\mathrm{tr}(A+B)&= \mathrm{tr}(A) + \mathrm{tr}(B) \\\\
\mathrm{tr}(AB) &= \mathrm{tr}(BA) \\\\
\mathrm{tr}(ABC) &= \mathrm{tr}(CAB) = \mathrm{tr}(BCA)
\end{align}
$$

## 2. 矩阵向量范数

- 矩阵$p$范数
  $$ \Vert A\Vert _p={(\sum\limits _{i=1}^m\sum\limits _{j=1}^n{| a_{ij}|}^p)}^{1/p}​$$
- 矩阵F范数

​          $$\Vert A\Vert _\mathrm F={(\sum\limits _{i=1}^m\sum\limits _{j=1}^n{a_{ij}}^2)}^{1/2} = {(\mathrm{tr}(A^\mathrm TA))}^{1/2} $$

- 矩阵1范数（列范数）：列上元素绝对值之和的最大者(列和最大)
- 矩阵无穷范数（行范数）：行上元素绝对值之和的最大者(行和最大)
- 矩阵的2范：矩阵$A^TA​$的最大特征值开平方根
- 向量1范：向量$x​$的各个元素的绝对值之和
  $$\Vert x\Vert _1 = \sum\limits _{i=1}^n\vert x _i\vert​$$ 
- 向量2范:向量的每个元素的平方和再开平方根
  $$ \Vert x\Vert _2={(\sum\limits _{i=1}^n{\vert{x_i}\vert}^2)}^{1/2}$$
- 向量的负无穷范数：向量的所有元素的绝对值中最小的：
  $$ \Vert x\Vert_{-\infty}=\min{|{x_i}|} ​$$
- 向量的正无穷范数：向量的所有元素的绝对值中最大的：
  $$ \Vert x\Vert _{+\infty}=\max{|{x_i}|} ​$$
- 向量的p范数：向量元素绝对值的p次方和的1/p次幂。
  $$ L_p=\Vert x\Vert _p={(\sum\limits_{i=1}^{n}|{x_i}|^p)}^{1/p} ​$$

## 3. 矩阵求导
### 3.1 常见矩阵的求导公式
$$
\begin{align}
\frac{\partial  x^\mathrm T }{\partial x} &= I \\\\
\frac{\partial \beta ^\mathrm T x}{\partial x} &= \beta \\\\
\frac{\partial x ^\mathrm T x}{\partial x} & = 2x  \\\\
\frac{\partial x ^\mathrm T Ax}{\partial x} & = \frac{\partial \mathrm {tr} (x ^\mathrm T Ax)}{\partial x}  
=\frac{\partial \mathrm {tr} (Axx ^\mathrm T)}{\partial x} = (A+A^\mathrm T)x \\\\
\frac{\partial \mathrm{tr}(AB)}{\partial A} &= B^\mathrm T \\\\
\frac{\partial \mathrm{tr}(ABA^\mathrm T C)}{\partial A} &= CAB + C^\mathrm T AB^\mathrm T  \\\\
\frac{\partial \mathrm{tr}(A)}{\partial A} & = I \\\\
\frac{\partial \mathrm{tr}(xy^\mathrm T)}{\partial x} &= \frac{\partial \mathrm {tr}(y^\mathrm Tx)}{\partial x}=y
\end{align}
$$
### 3.2 几种重要矩阵
- **梯度矩阵(Gradient Matrix)**:标量函数对列向量的导数，为列向量 
- **雅克比矩阵(Jacobian Matrix)**： 列向量函数对行向量的导数矩阵
- **海参矩阵(Hessian Matrix)**:标量函数对向量的二阶微分矩阵

## 4. 矩阵分解
- SVD分解 
$$A = U\Sigma V^\mathrm T​$$
- 特征值分解，特征值与特征根：$Av_i = \lambda _i v_i$ 其中 $v_i$为特征值$\lambda _i$所对应的特征向量

## 5. 概率基础

## 6. 优化理论

连续可微函数的泰勒展开式
$$
f(x+\Delta x) = f(x) + f'(x)\Delta x + \frac{f''(x)}{2!}\Delta x^2 + \cdots + \frac{f^{(n)}(x)}{n!}\Delta x^n + \cdots
$$
多元变量连续可微函数的泰勒展开式：
$$
f(x+\Delta x) = f(x) + \nabla f(x)\Delta x + \Delta x^\mathrm T \frac{\nabla^2 f(x)}{2}\Delta x + \cdots
$$

- 梯度下降：欲满足$f(x+\Delta x) < f(x)​$，选择$\Delta x = -\gamma \nabla f(x) ​$($\gamma​$为小常数步长)，即梯度下降的方向。

- 牛顿法：优化函数用泰勒展开的二阶近似表示，极值点$x^*$满足一阶导为0即$\nabla f(x^*) = 0$，所以其更新公式为：
  $$
  x_{k+1} = x_k - \mathrm H^{-1}(x_k)\nabla f(x_k)
  $$

- 拟牛顿法：牛顿法需要计算二阶海森(Hessian)矩阵，代价比较大。拟牛顿饭寻找海森矩阵的快速近似计算方法

- 拉格朗日优化：

  一般无约束优化问题： $\min\limits _x f(x)$。

  找到一阶导为0的点即$\nabla _x f(x) = 0$。若无解析解，可以使用梯度下降或者牛顿饭等方法逐步逼近极值点

  一般约束优化问题：
  $$
  \begin{align}
  \min\limits _x:& f(x) \\\\
  s.t:& h_i(x) = 0, i=1,2\cdots,m\\\\
  &g_i(x) \le 0,i=1,2\cdots,n 
  \end{align}
  $$
  引入拉格朗日算法。
  $$
  \begin{align}
  \min\limits_{x,\alpha,\beta}:&L(x,\alpha,\beta) = f(x) + \sum\limits_{i=1}^m\alpha_i h_i(x) + \sum\limits_{i=1}^n \beta_ig_i(x)\\\\
  \end{align}
  $$
  上述优化问题的KKT条件：
  $$
  \begin{eqnarray}
  \nabla_xL(X,\alpha,\beta) &=& 0 \\\\
  \beta_jg_j(x) &=& 0, j = 1,2,\cdots,n \\\\
  g_j(x) &\le& 0, j = 1,2,\cdots,n \\\\
  \beta_j &\ge& 0,j = 1,2,\cdots,n \\\\
  h_i(x) &=& 0, i = 1,2,\cdots,m\\\\
  \end{eqnarray}
  $$
  

- 梯度下降

- 

## 7.  
