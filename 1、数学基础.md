# 数学基础
***
常见记号
向量$x = {[x_1,x_2,\cdots,x_n]}^{\mathrm{T}} ​$
矩阵 $A = [a_1,a_2,\cdots,a_n] = (a_{ij}) ​$，其中$a_i(i=1,\cdots,n）​$为列向量

## 1. 矩阵迹
**定义**：$n​$阶方阵$A​$主对角线上元素之和，即$\mathrm{tr}(A)=\sum _{i=1}^na_{ii}​$。
$$ \begin{align}
\mathrm{tr}(A^{\mathrm T})&= \mathrm{tr}(A)  \<span class=""></span>\
\mathrm{tr}(A+B)&= \mathrm{tr}(A) + \mathrm{tr}(B) \<span class=""></span>\
\mathrm{tr}(AB) &= \mathrm{tr}(BA) \<span class=""></span>\
\mathrm{tr}(ABC) &= \mathrm{tr}(CAB) = \mathrm{tr}(BCA)
\end{align} ​$$

## 2. 矩阵向量范数
- 矩阵$p$范数
  $$ \Vert A\Vert _p={(\sum\limits _{i=1}^m\sum\limits _{j=1}^n{| a_{ij}|}^p)}^{1/p}$$
- 矩阵F范数

​          $$\Vert A\Vert _F={(\sum\limits _{i=1}^m\sum\limits _{j=1}^n{a_{ij}}^2)}^{1/2} = {(\mathrm{tr}(A^\mathrm TA))}^{1/2} ​$$

- 矩阵1范数（列范数）：列上元素绝对值之和的最大者(列和最大)
- 矩阵无穷范数（行范数）：行上元素绝对值之和的最大者(行和最大)
- 矩阵的2范：矩阵$A^TA​$的最大特征值开平方根
- 向量1范：向量$x​$的各个元素的绝对值之和
  $$\Vert x\Vert _1 = \sum\limits _{i=1}^n\vert x _i\vert​$$ 
- 向量2范:向量的每个元素的平方和再开平方根
  $$ \Vert x\Vert _2={(\sum\limits _{i=1}^n{\vert{x_i}\vert}^2)}^{1/2}$$
- 向量的负无穷范数：向量的所有元素的绝对值中最小的：
  $$ \Vert x\Vert_{-\infty}=\min{|{x_i}|} ​$$
- 向量的正无穷范数：向量的所有元素的绝对值中最大的：
  $$ \Vert x\Vert _{+\infty}=\max{|{x_i}|} ​$$
- 向量的p范数：向量元素绝对值的p次方和的1/p次幂。
  $$ L_p=\Vert x\Vert _p={(\sum\limits_{i=1}^{n}|{x_i}|^p)}^{1/p} $$

## 3. 矩阵求导
### 3.1 常见矩阵的求导公式
$$
\begin{align}
\frac{\partial  x^\mathrm T }{\partial x} &= I \\\\
\frac{\partial \beta ^\mathrm T x}{\partial x} &= \beta \\\\
\frac{\partial x ^\mathrm T x}{\partial x} & = 2x  \\\\
\frac{\partial x ^\mathrm T Ax}{\partial x} & = \frac{\partial \mathrm {tr} (x ^\mathrm T Ax)}{\partial x}  
=\frac{\partial \mathrm {tr} (Axx ^\mathrm T)}{\partial x} = (A+A^\mathrm T)x \\\\
\frac{\partial \mathrm{tr}(AB)}{\partial A} &= B^\mathrm T \\\\
\frac{\partial \mathrm{tr}(ABA^\mathrm T C)}{\partial A} &= CAB + C^\mathrm T AB^\mathrm T  \\\\
\frac{\partial \mathrm{tr}(A)}{\partial A} & = I \\\\
\frac{\partial \mathrm{tr}(xy^\mathrm T)}{\partial x} &= \frac{\partial \mathrm {tr}(y^\mathrm Tx)}{\partial x}=y
\end{align}
$$
### 3.2 几种重要矩阵
- **梯度矩阵(Gradient Matrix)**:标量函数对列向量的导数，为列向量 
- **雅克比矩阵(Jacobian Matrix)**： 列向量函数对行向量的导数矩阵
- **海参矩阵(Hessian Matrix)**:标量函数对向量的二阶微分矩阵

## 4. 矩阵分解
- SVD分解 
$$A = U\Sigma V^\mathrm T​$$
- 特征值分解，特征值与特征根：$Av_i = \lambda _i v_i$ 其中 $v_i$为特征值$\lambda _i$所对应的特征向量

## 5. 概率基础

## 6. 优化理论

## 7.  
