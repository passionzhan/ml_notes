# 数学基础
***
常见记号
向量$x = {[x_1,x_2,\cdots,x_n]}^{\mathrm{T}} $
矩阵 $A = [a_1,a_2,\cdots,a_n] = (a_{ij}) $，其中$a_i(i=1,\cdots,n）$为列向量

## 1. 矩阵迹
**定义**：$n$阶方阵$A$主对角线上元素之和，即$\mathrm{tr}(A)=\sum _{i=1}^na_{ii}$。
$$
\begin{align}
\mathrm{tr}(A^{\mathrm T})&= \mathrm{tr}(A)  \\\\
\mathrm{tr}(A+B)&= \mathrm{tr}(A) + \mathrm{tr}(B) \\\\
\mathrm{tr}(AB) &= \mathrm{tr}(BA) \\\\
\mathrm{tr}(ABC) &= \mathrm{tr}(CAB) = \mathrm{tr}(BCA)
\end{align}
$$

## 2. 矩阵向量范数

- 矩阵$p$范数
  $$ \Vert A\Vert _p={(\sum\limits _{i=1}^m\sum\limits _{j=1}^n{| a_{ij}|}^p)}^{1/p}$$
- 矩阵F范数

​          $$\Vert A\Vert _\mathrm F={(\sum\limits _{i=1}^m\sum\limits _{j=1}^n{a_{ij}}^2)}^{1/2} = {(\mathrm{tr}(A^\mathrm TA))}^{1/2} $$

- 矩阵1范数（列范数）：列上元素绝对值之和的最大者(列和最大)
- 矩阵无穷范数（行范数）：行上元素绝对值之和的最大者(行和最大)
- 矩阵的2范：矩阵$A^TA$的最大特征值开平方根
- 向量1范：向量$x$的各个元素的绝对值之和
  $$\Vert x\Vert _1 = \sum\limits _{i=1}^n\vert x _i\vert$$ 
- 向量2范:向量的每个元素的平方和再开平方根
  $$ \Vert x\Vert _2={(\sum\limits _{i=1}^n{\vert{x_i}\vert}^2)}^{1/2}$$
- 向量的负无穷范数：向量的所有元素的绝对值中最小的：
  $$ \Vert x\Vert_{-\infty}=\min{|{x_i}|} $$
- 向量的正无穷范数：向量的所有元素的绝对值中最大的：
  $$ \Vert x\Vert _{+\infty}=\max{|{x_i}|} $$
- 向量的p范数：向量元素绝对值的p次方和的1/p次幂。
  $$ L_p=\Vert x\Vert _p={(\sum\limits_{i=1}^{n}|{x_i}|^p)}^{1/p} $$

## 3. 矩阵求导
### 3.1 常见矩阵的求导公式
$$
\begin{align}
\frac{\partial  x^\mathrm T }{\partial x} &= I \\\\
\frac{\partial \beta ^\mathrm T x}{\partial x} &= \beta \\\\
\frac{\partial x ^\mathrm T x}{\partial x} & = 2x  \\\\
\frac{\partial x ^\mathrm T Ax}{\partial x} & = \frac{\partial \mathrm {tr} (x ^\mathrm T Ax)}{\partial x}  
=\frac{\partial \mathrm {tr} (Axx ^\mathrm T)}{\partial x} = (A+A^\mathrm T)x \\\\
\frac{\partial \mathrm{tr}(AB)}{\partial A} &= B^\mathrm T \\\\
\frac{\partial \mathrm{tr}(ABA^\mathrm T C)}{\partial A} &= CAB + C^\mathrm T AB^\mathrm T  \\\\
\frac{\partial \mathrm{tr}(A)}{\partial A} & = I \\\\
\frac{\partial \mathrm{tr}(xy^\mathrm T)}{\partial x} &= \frac{\partial \mathrm {tr}(y^\mathrm Tx)}{\partial x}=y
\end{align}
$$
### 3.2 几种重要矩阵
- **梯度矩阵(Gradient Matrix)**:标量函数对列向量的导数，为列向量 
- **雅克比矩阵(Jacobian Matrix)**： 列向量函数对行向量的导数矩阵
- **海参矩阵(Hessian Matrix)**:标量函数对向量的二阶微分矩阵

## 4. 矩阵分解
- SVD分解 
$$A = U\Sigma V^\mathrm T$$
- 特征值分解，特征值与特征根：$Av_i = \lambda _i v_i$ 其中 $v_i$为特征值$\lambda _i$所对应的特征向量

## 5. 概率基础

### 5.1 概率分布

- 均匀分布（Uniform Distribution）

  - 概率分布：$p(x|a,b) = \frac{1}{b-a}$
  - 期望：$\mathrm E[x] = \frac{a+b}{2}$
  - $\mathrm{var} [x] = \frac{(b-a)^2}{12}$

- 伯努利分布(Bernoulli Distribution):关于布尔变量$x\in\{0,1\}$的概率分布，参数$\mu\in[0,1]$表示变量$x=1$的概率:
  $$
  \begin{align}
  P(x|\mu) &= \mathrm{Bern}(x|\mu) = \mu^x(1-\mu)^{1-x} \\\\
  \mathrm E[x] &= \mu \\\\
  \mathrm{var}[x] &= \mu(1-\mu) 
  \end{align}
  $$

- 二项分布(Binomial Distribution)用以描述$N$次独立的伯努利实验中有$m$次成功(即$x=1$)的概率。
  $$
  \begin{align}
  P(m|N,\mu) &= Bin(m|N,\mu) = \left(\begin{array}{c}N \\\\m\end{array}\right)\mu^m(1-\mu)^{N-m} \\\\
  \mathrm E(x) &= N\mu \\\\
  \mathrm{var}(x) &= N\mu(1-\mu)
  \end{align}
  $$

- 多项分布，将伯努利分布由单变量扩展到多变量$x$,其中$x_i\in \{0,1\}$且$\sum_{i=1}^d x_i= 1$,并假设$x_i$取1的概率为$\mu_i\in [0,1],\sum_{i=1}^d \mu_i= 1$.
  $$
  \begin{align}
  P(x|\mu) &= \prod\limits_{i=1}^d\mu_i^{x_i} \\\\
  \mathrm E[x_i] &= \mu_i \\\\
  \mathrm{var}[x_i] &= \mu_i(1-\mu_i) \\\\
  cov[x_j,x_i] &= \mathrm I[x_i=x_j]\mu_i 
  \end{align}
  $$
  **备注**：机器学习中常用上述分布描述多类分类问题。$x_i = 1$表示对应样本属于第$i$类。若用单变量表示分类结果，即用$x=i(i=1,2,\cdots,d)$表示样本属于第$i$类，则$P(x=i|\mu) = \prod_{i=1}^d\mu_i^{\mathrm I[x=i]}$

- 高斯分布（$n$元正态分布，Normal Distribution）：
  $$
  \begin{align}
  P(x|\mu,\Sigma) &= N(x|\mu,\Sigma) = \frac{1}{\sqrt{(2\pi)^n|\Sigma|}}\exp\left(-\frac{1}{2}(x-\mu)^\mathrm T\Sigma^{-1}(x-\mu)\right)\\\\ 
  \mathrm E[x] &= \mu\\\\
  \mathrm {cov}[x] &= \Sigma
  \end{align}
  $$

- 其他分布……


### 5.2 最大似然估计



## 6. 优化理论

连续可微函数的泰勒展开式
$$
f(x+\Delta x) = f(x) + f'(x)\Delta x + \frac{f''(x)}{2!}\Delta x^2 + \cdots + \frac{f^{(n)}(x)}{n!}\Delta x^n + \cdots
$$
多元变量连续可微函数的泰勒展开式：
$$
f(x+\Delta x) = f(x) + \nabla f(x)\Delta x + \Delta x^\mathrm T \frac{\nabla^2 f(x)}{2}\Delta x + \cdots
$$

- 梯度下降：欲满足$f(x+\Delta x) < f(x)$，选择$\Delta x = -\gamma \nabla f(x) $($\gamma$为小常数步长)，即梯度下降的方向。

- 牛顿法：优化函数用泰勒展开的二阶近似表示，极值点$x^*$满足一阶导为0即$\nabla f(x^*) = 0$，所以其更新公式为：
  $$
  x_{k+1} = x_k - \mathrm H^{-1}(x_k)\nabla f(x_k)
  $$

- 拟牛顿法：牛顿法需要计算二阶海森(Hessian)矩阵，代价比较大。拟牛顿饭寻找海森矩阵的快速近似计算方法

- 拉格朗日优化：

  一般无约束优化问题： $\min\limits _x f(x)$。

  找到一阶导为0的点即$\nabla _x f(x) = 0$。若无解析解，可以使用梯度下降或者牛顿法等方法逐步逼近极值点

  一般约束优化问题：
  $$
  \begin{align}
  \min\limits _x:& f(x) \\\\
  s.t:& h_i(x) = 0, i=1,2\cdots,m\\\\
  &g_i(x) \le 0,i=1,2\cdots,n 
  \end{align}
  $$
  引入拉格朗日算法。
  $$
  \begin{align}
  \min\limits_{x,\alpha,\beta}:&L(x,\alpha,\beta) = f(x) + \sum\limits_{i=1}^m\alpha_i h_i(x) + \sum\limits_{i=1}^n \beta_ig_i(x)\\\\
  \end{align}
  $$
  上述优化问题的KKT条件：
  $$
  \begin{align}
  \nabla_xL(X,\alpha,\beta) & = 0 \tag{1}\\\\
  \beta_jg_j(x) &= 0, j = 1,2,\cdots,n \tag{2}\\\\
  g_j(x) &\le 0, j = 1,2,\cdots,n \tag{3}\\\\
  \beta_j &\ge 0,j = 1,2,\cdots,n \tag{4}\\\\
  h_i(x) &= 0, i = 1,2,\cdots,m\tag{5}\\\\
  \end{align}
  $$
  其中公式（2）称为松弛互补条件

## 7.  其他概念

随机变量$x$的$n$个取值的概率记为$p_i$。

样本集合$D$包含$k$类样本，没了样本所占比例为$p_i$。

- 信息熵:描述变量$x$取值的不确定性。$H(x)= \mathrm E[-\log p_i] = -\sum_{i=1}^np_i\log p_i$

  样本集$D$的信息熵定义为: $Ent(D) = -\sum_{i=1}^k p_i\log p_i$

- 条件信息熵

- 互信息

- 信息增益

- 基尼系数（Gini ）

- KL散度

- 似然函数

- 