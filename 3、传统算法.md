# 传统算法

 训练数据集$T = \{(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\},x_i\in \Bbb R^D,y\in\{+1,-1\}$

1. ###  感知机

   给定上述训练数据集，感知机算法寻找如下决策函数$y = f(x) = sign (wx+b)$.

   原始感知机算法($\eta$为学习率)：

   ***

   1. 初始化：$w, b$

   2. 在训练集中寻找误分类样本$(x_i,y_i)$,即$y_i(wx_i+b_i) \le 0 $

      更新$w,b$:
      $$
      \begin{align}
      w &= w + \eta y_ix_i\\\\
      b &= b + \eta y_i
      \end{align}
      $$

   3. 重复步骤（2），直到没有误分类样本，终止。

   ***

   一般学习问题对偶形式的基本思想是将待学习参数$w, b$表示成训练样本$x_i$和标记$y_i$的线性组合形式，通过求解其组合系数求得$w, b$。在上述原始感知机学习算法中，初始化时，若设$w=0,b=0$，则最后学习得到的$w, b$具有如下形式：
   $$
   \begin{align}
   w &= \sum_{i=1}^n\alpha_iy_ix_i\\\\
   b &= \sum_{i=1}^n \alpha_iy_i
   \end{align}
   $$
   上述表示模式称为感知机学习算法的对偶模式。

   感知机学习算法的对偶形式：

   ***

   输出：$\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_n)$；感知机模型为$f(x) =sign(\sum\limits_{i=1}^n\alpha_iy_ix_i\cdot x + \sum\limits_{i=1}^n\alpha_iy_i)$

   1. 初始化 $\alpha = 0$

   2. 训练集中选取误分类样本$(x_j,y_j)$即$y_j(\sum\limits_{i=1}^n\alpha_iy_ix_i\cdot x_j + \sum\limits_{i=1}^n\alpha_iy_i)\le 0$
      $$
      \alpha_j =\alpha_j + \eta
      $$

   3. 重复步骤（2），直到没有误分类样本，终止。

   ***

2. ### $k$近邻算法

   $k$近邻分类决策规则是多数表决(majority voting rule)，即由输入实例的$k$个近邻训练实例的多数类决定输入实例的类别。这其实是一种经验风险最小化的决策规则。假设分类损失函数为0-1损失函数，$k$近邻分类函数为$f: \Bbb R^n \rightarrow \{c_1,c_2,\cdots,c_K\}$

   误分类概率为：
   $$
   P(Y \ne f(X)) = 1 - P(Y = f(X))
   $$
   给定实例$x\in \cal X$,其$k$近邻训练实例集合记为$\cal N_k(x)$，设涵盖$\cal N_k(x)$区域的类别为$c_j$($x$的真实类别)，则误分类概率为：
   $$
   \frac{1}{k}\sum_{x_i\in \cal N_k(x)}\Bbb I(y_i \ne c_j) = 1 - \frac{1}{k}\sum_{x_i\in \cal N_k(x)}\Bbb I(y_i = c_j)
   $$
   要使误分类概率即经验风险最小，就要使$\frac{1}{k}\sum_{x_i\in \cal N_k(x)}\Bbb I(y_i = c_j)$最大，所以多数投票规则等于经验风险最小化。

   实现$k$近邻算法的关键是如何对训练数据进行快速的近邻检索。

   **关键点**：1、构造平衡kd树来实现。2、基于kd树的近邻检索。

3. ### 决策树

4. ###  线性回归

   $T = \{(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\},x_i\in \Bbb R^D,y\in \Bbb R$

   线性回归问题通过上述数据训练寻找合适的线性预测函数$f(x) = w^\mathrm Tx + b$来预测样本$x$的$y$值。线性回归通过最小化如下平方和误差来求解$w, b$:
   $$
   \min_{w,b} L(Y,F(x)) = \frac{1}{2}\sum_{i=1}^n(f(x_i)-y_i)^2 = \frac{1}{2}\sum_{i=1}^n(w^\mathrm Tx_i+b-y_i)^2 = \frac{1}{2}\|w^\mathrm TX + b - Y\|^2
   $$
   上述优化问题存在解析解.令$\bar w = (w^\mathrm T,b)^\mathrm T, \bar X = (X^\mathrm T,1)^\mathrm T$,则解析为$\bar w = (\bar X\bar X^\mathrm T)^{-1}\bar XY^\mathrm T$

   **广义线性模型**，假设$g$为单调递增可微连续函数，$y =g^{-1} (w^\mathrm Tx + b)$ 为广义线性模型。

5. ### 主成分分析

6. ### 线性判别分析

7. ### 流形学习

8. ###  逻辑斯蒂回归

9. ###  支持向量机

10. ###  期望最大化(EM)算法

11. ###  贝叶斯估计

12. ###  集成学习

13. ###  神经网络

14. ###  最大熵模型

15. ###  隐马尔可夫模型

16. ###  条件随机场







