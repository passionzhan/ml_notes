# 传统算法

 训练数据集$T = \{(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\},x_i\in \Bbb R^D,y\in\{+1,-1\}$

1. ###  感知机

   给定上述训练数据集，感知机算法寻找如下决策函数$y = f(x) = sign (wx+b)$.

   原始感知机算法($\eta$为学习率)：

   ***

   1. 初始化：$w, b$

   2. 在训练集中寻找误分类样本$(x_i,y_i)$,即$y_i(wx_i+b_i) \le 0 $

      更新$w,b$:
      $$
      \begin{align}
      w &= w + \eta y_ix_i\\\\
      b &= b + \eta y_i
      \end{align}
      $$

   3. 重复步骤（2），直到没有误分类样本，终止。

   ***

   一般学习问题对偶形式的基本思想是将待学习参数$w, b$表示成训练样本$x_i$和标记$y_i$的线性组合形式，通过求解其组合系数求得$w, b$。在上述原始感知机学习算法中，初始化时，若设$w=0,b=0$，则最后学习得到的$w, b$具有如下形式：
   $$
   \begin{align}
   w &= \sum_{i=1}^n\alpha_iy_ix_i\\\\
   b &= \sum_{i=1}^n \alpha_iy_i
   \end{align}
   $$
   上述表示模式称为感知机学习算法的对偶模式。

   感知机学习算法的对偶形式：

   ***

   输出：$\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_n)$；感知机模型为$f(x) =sign(\sum\limits_{i=1}^n\alpha_iy_ix_i\cdot x + \sum\limits_{i=1}^n\alpha_iy_i)$

   1. 初始化 $\alpha = 0$

   2. 训练集中选取误分类样本$(x_j,y_j)$即$y_j(\sum\limits_{i=1}^n\alpha_iy_ix_i\cdot x_j + \sum\limits_{i=1}^n\alpha_iy_i)\le 0$
      $$
      \alpha_j =\alpha_j + \eta
      $$

   3. 重复步骤（2），直到没有误分类样本，终止。

   ***

2. ### $k$近邻算法

   $k$近邻分类决策规则是多数表决(majority voting rule)，即由输入实例的$k$个近邻训练实例的多数类决定输入实例的类别。这其实是一种经验风险最小化的决策规则。假设分类损失函数为0-1损失函数，$k$近邻分类函数为$f: \Bbb R^n \rightarrow \{c_1,c_2,\cdots,c_K\}$

   误分类概率为：
   $$
   P(Y \ne f(X)) = 1 - P(Y = f(X))
   $$
   给定实例$x\in \cal X$,其$k$近邻训练实例集合记为$\cal N_k(x)$，设涵盖$\cal N_k(x)$区域的类别为$c_j$($x$的真实类别)，则误分类概率为：
   $$
   \frac{1}{k}\sum_{x_i\in \cal N_k(x)}\Bbb I(y_i \ne c_j) = 1 - \frac{1}{k}\sum_{x_i\in \cal N_k(x)}\Bbb I(y_i = c_j)
   $$
   要使误分类概率即经验风险最小，就要使$\frac{1}{k}\sum_{x_i\in \cal N_k(x)}\Bbb I(y_i = c_j)$最大，所以多数投票规则等于经验风险最小化。

   实现$k$近邻算法的关键是如何对训练数据进行快速的近邻检索。

   **关键点**：1、构造平衡kd树来实现。2、基于kd树的近邻检索。

3. ### 朴素贝叶斯分类

   朴素贝叶斯方法通过如下公式学习数据的联合分布
   $$
   P(X,Y) = P(X|Y)P(Y)
   $$
    然后通过最大后验概率确定数据的类别，后验概率计算根据贝叶斯定理进行：
   $$
   \begin{align}
   P(Y=c_k|X=x) &= \frac{P(X=x,Y = c_k)}{P(X = x)} = \frac{P(X = x|Y=c_k)P(Y=c_k)}{\sum_k P(X = x|Y=c_k)P(Y=c_k)} \\\\
   &\propto {P(X = x|Y=c_k)P(Y=c_k)}
   \end{align}
   $$
   为简化计算，对上述条件概率做条件独立性假设，最终朴素贝叶斯分类器可以表示成：
   $$
   \begin{align}
   y = f(x) &= \arg \max_{c_k} P(X = x|Y=c_k)P(Y=c_k) \\\\
   & = P(Y=c_k)\prod_i P(X^{(i)} = x^{(i)}|Y=c_k)
   \end{align}
   $$
   **注意**：

   1. 朴素贝叶斯方法首先学习数据的联合概率分布，所以其属于生成模型
   2. 通过设置0-1损失函数，朴素贝叶斯方法中的后验概率最大化等价于期望风险最小化。

   朴素贝叶斯方法中参数条件概率(**先验概率**)可以通过**最大似然进行估计**，即
   $$
   \begin{align}
   P(Y=c_k) &= \frac{\sum _{i=1}^n\Bbb I[y_i = c_k]}{n} \\\\
   P(X^{(j)} = a_{jl}|Y=c_k) &=  \frac{\sum _{i=1}^n\Bbb I[y_i = c_k, x_i^{(j)} = a_{jl}]}{\sum _{i=1}^n\Bbb I[y_i = c_k]}
   \end{align}
   $$
   利用极大似然估计可能出现所要估计的值为0的情况，一般采用贝叶斯估计来解决此问题（$K$为总类别数）。
   $$
   \begin{align}
   P_\lambda (Y=c_k) &= \frac{\sum _{i=1}^n\Bbb I[y_i = c_k] + \lambda}{n + K\lambda}\\\\
   P_\lambda (X^{(j)} = a_{jl}|Y=c_k) &=  \frac{\sum _{i=1}^n\Bbb I[y_i = c_k, x_i^{(j)}= a_{jl}] + \lambda}{\sum _{i=1}^n\Bbb I[y_i = c_k] + S_j\lambda} 
   \end{align}
   $$
   上述公式中$\lambda>0$，当$\lambda=1$时又称拉普拉斯平滑（Laplace Smoothing）

4. ### 决策树

   $D$ 样本集合、类别数为$K$

   决策树(Decision Tree)是一种基本的分类与回归方法。常用决策树算法有 ID3、C4.5、CART。

   决策树的关键是如何选择每次划分的特征，一般采用启发式方法。常见启发式函数有:

   #### 1. ID3——最大信息增益：    
   决策前经验信息熵：
   $$
   H(D) = -\sum_{k=1}^K\frac{|C_k|}{|D|}\log\frac{|C_k|}{|D|}
   $$
   $C_k$ 表示D中第$k$类样本子集。
   特征A对数据集$D$的信息熵 H(D|A)为：
   $$
   H(D|A) = \sum_{i=1}^{A_l}\frac{|D_i|}{|D|}H(D_i) =  \sum_{i=1}^{A_l}\frac{|D_i|}{|D|}(-\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}\log\frac{|D_{ik}|}{|D_i|})
   $$
   $A_l$为特征A取值个数，$D_i$ 表示特征A 取第$i$ 个值的样本子集。$D_{ik}$ 表示$D_i$ 中属于$k$ 类中样本子集。
   信息增益：
   $$
   G(D,A) = H(D) - H(D|A)
   $$

   #### 2. C4.5——最大信息增益比
   特征A 对数据集D 的信息增益比定义为：
   $$
   G_R(D,A) = \frac{G(D,A)}{H_A(D)}
   $$

   其中$H_A(D)= -\sum_{i=1}^{A_l}\frac{|D_i|}{|D|}\log\frac{|D_i|}{|D|}$，称为 数据集D关于A的取值熵。

   #### 3. CART——基尼指数（Gini）
   分类与回归树CART (Classification and Regression Tree)。在回归问题中，采用平方误差最小化的准则进行特征选择，而在分类问题中采用最小化基尼指数的原则进行特征选择。
   Gini 描述的是数据的纯度，与信息熵含义类似：
   $$
   \mathrm {Gini}(D) = 1 - \sum_{i=1}^K{(\frac{|C_k|}{|D|})}^2
   $$
   特征A 对应的Gini指数对应为：
   $$
   \mathrm{Gini}(D|A) = \sum_{i=1}^{A_l}\frac{|D_i|}{|D|}\mathrm{Gini}(D_i)
   $$

   CART是一颗二叉树，采用二元切分法，因此上述公式中只对两项进行求和。CART在每次迭代过程中选Gini指数最小的特征及其对应的特征值作为切分点进行分类。

5. ###  线性回归

   $T = \{(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\},x_i\in \Bbb R^D,y\in \Bbb R$

   线性回归问题通过上述数据训练寻找合适的线性预测函数$f(x) = w^\mathrm Tx + b$来预测样本$x$的$y$值。线性回归通过最小化如下平方和误差来求解$w, b$:
   $$
   \min_{w,b} L(Y,F(x)) = \frac{1}{2}\sum_{i=1}^n(f(x_i)-y_i)^2 = \frac{1}{2}\sum_{i=1}^n(w^\mathrm Tx_i+b-y_i)^2 = \frac{1}{2}\|w^\mathrm TX + b - Y\|^2
   $$
   上述优化问题存在解析解.令$\bar w = (w^\mathrm T,b)^\mathrm T, \bar X = (X^\mathrm T,1)^\mathrm T$,则解析为$\bar w = (\bar X\bar X^\mathrm T)^{-1}\bar XY^\mathrm T$

   **广义线性模型**，假设$g$为单调递增可微连续函数，$y =g^{-1} (w^\mathrm Tx + b)$ 为广义线性模型。

6. ### 主成分分析

7. ### 线性判别分析

8. ### 流形学习

   - 多维缩放(MDS):
   - 等距特征映射(ISOMAP)
   - 局部线性嵌入(LLE, Locally Linear Embedding)
   - 局部切空间对齐(LTSA, Local tangent Space Alignment)
   - 拉普拉斯特征映射(LE, Laplacian Eigenmaps)

9. ###  逻辑斯蒂回归

   逻辑斯蒂回归(Logistic Regression)解决的是分类问题。令$\mu = p(y=1|x)$即样本属于正例的概率，则$1-\mu$表示样本属于负类的概率。LR认为对数几率(log odds)或logit函数是变量$x$的线性函数（广义线性模型），即：
   $$
   \mathrm{logit}(\mu) = \log \frac{\mu}{1-\mu} = w^\mathrm{T}x+b
   $$
   从而:
   $$
   \mu = \frac{1}{1+\exp {(-(w^\mathrm{T}x+b)})}
   $$
   不是一般性，记$w=(w,b), x=(x,1)$。一般应用最大似然法来估计上述模型的参数。设$P(Y=1|x）= \pi(x), P(Y=0|x) = 1-\pi(x)$, 则训练数据的似然函数为:
   $$
   \prod_{i=1}^n\pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i}
   $$
   对数似然函数为：
   $$
   \begin{align}L(w) &= \sum_{i=1}^n[y_i\log\pi(x_i) + (1-y_i)\log(1-\pi(x_i))] \\\\
   &= \sum_{i=1}^n[y_i\log\frac{\pi(x_i)}{1-\pi(x_i)} + \log(1-\pi(x_i))]\\\\
   &= \sum_{i=1}^n[y_i(w\cdot x_i) - \log (1+\exp(w\cdot x_i)) 
   \end{align}
   $$
   对$L(w)$求极大值得到$w$的估计，可以采用梯度下降法或者拟牛顿法。

   #### 注意：

   - 最大化上述对数似然函数，等价于最小化对数损失函数：

   $$
   \begin{align}
   L(w,x,y) &= -\log P(y|x) = 
   \begin{cases}
   -\log P(y=1|x) = -y\log\pi(x), &y=1\\\\
   -\log P(y=0|x)  = -(1-y)\log(1-\pi(x)), & y=0
   \end{cases}
    \\\\
    &= y\log\pi(x) + (1-y)\log(1-\pi(x))
   \end{align}
   $$

   - 最大化上述对数似然函数，等价于最小化标签的真实分布和标签的条件分布之间的**KL散度**。
     $$
     KL(y,p(y|x)) = \sum_y ylogP(y|x) = y\log\pi(x) + (1-y)\log(1-\pi(x))
     $$

   逻辑斯蒂回归是二类分类模型，可将其推广到多项逻辑斯蒂回归模型。假设离散类型变量$Y$取值为集合$\{1,2,\cdots,K\}$，那么多项逻辑斯蒂回归模型的为：
   $$
   \begin{align}P(Y=k|x) &= \frac{\exp(w_k\cdot x)}{1+\sum_{i=1}^{K-1}\exp(w_k\cdot x)} \\\\
   P(Y=K|x) &= \frac{1}{1+\sum_{i=1}^{K-1}\exp(w_k\cdot x)}
   \end{align}
   $$
   然后通过最大化对数似然函数或者最小化对数损失函数、最小化KL散度等方法对模型参数进行估计。

10. ###  支持向量机

11. ###  期望最大化(EM)算法

12. ###  集成学习

13. ###  神经网络

14. ###  最大熵模型

15. ###  隐马尔可夫模型

16. ###  条件随机场







