# 传统算法

 训练数据集$T = \{(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\},x_i\in \Bbb R^D,y\in\{+1,-1\}$

1. ###  感知机

   给定上述训练数据集，感知机算法寻找如下决策函数$y = f(x) = sign (wx+b)$.

   原始感知机算法($\eta$为学习率)：

   ***

   1. 初始化：$w, b$

   2. 在训练集中寻找误分类样本$(x_i,y_i)$,即$y_i(wx_i+b_i) \le 0 $

      更新$w,b$:
      $$
      \begin{align}
      w &= w + \eta y_ix_i\\\\
      b &= b + \eta y_i
      \end{align}
      $$

   3. 重复步骤（2），直到没有误分类样本，终止。

   ***

   一般学习问题对偶形式的基本思想是将待学习参数$w, b$表示成训练样本$x_i$和标记$y_i$的线性组合形式，通过求解其组合系数求得$w, b$。在上述原始感知机学习算法中，初始化时，若设$w=0,b=0$，则最后学习得到的$w, b$具有如下形式：
   $$
   \begin{align}
   w &= \sum_{i=1}^n\alpha_iy_ix_i\\\\
   b &= \sum_{i=1}^n \alpha_iy_i
   \end{align}
   $$
   上述表示模式称为感知机学习算法的对偶模式。

   感知机学习算法的对偶形式：

   ***

   输出：$\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_n)$；感知机模型为$f(x) =sign(\sum\limits_{i=1}^n\alpha_iy_ix_i\cdot x + \sum\limits_{i=1}^n\alpha_iy_i)$

   1. 初始化 $\alpha = 0$

   2. 训练集中选取误分类样本$(x_j,y_j)$即$y_j(\sum\limits_{i=1}^n\alpha_iy_ix_i\cdot x_j + \sum\limits_{i=1}^n\alpha_iy_i)\le 0$
      $$
      \alpha_j =\alpha_j + \eta
      $$

   3. 重复步骤（2），直到没有误分类样本，终止。

   ***

2. ### $k$近邻算法

   $k$近邻分类决策规则是多数表决(majority voting rule)，即由输入实例的$k$个近邻训练实例的多数类决定输入实例的类别。这其实是一种经验风险最小化的决策规则。假设分类损失函数为0-1损失函数，$k$近邻分类函数为$f: \Bbb R^n \rightarrow \{c_1,c_2,\cdots,c_K\}$

   误分类概率为：
   $$
   P(Y \ne f(X)) = 1 - P(Y = f(X))
   $$
   给定实例$x\in \cal X$,其$k$近邻训练实例集合记为$\cal N_k(x)$，设涵盖$\cal N_k(x)$区域的类别为$c_j$($x$的真实类别)，则误分类概率为：
   $$
   \frac{1}{k}\sum_{x_i\in \cal N_k(x)}\Bbb I(y_i \ne c_j) = 1 - \frac{1}{k}\sum_{x_i\in \cal N_k(x)}\Bbb I(y_i = c_j)
   $$
   要使误分类概率即经验风险最小，就要使$\frac{1}{k}\sum_{x_i\in \cal N_k(x)}\Bbb I(y_i = c_j)$最大，所以多数投票规则等于经验风险最小化。

   实现$k$近邻算法的关键是如何对训练数据进行快速的近邻检索。

   **关键点**：1、构造平衡kd树来实现。2、基于kd树的近邻检索。

3. ### 朴素贝叶斯分类

   朴素贝叶斯方法通过如下公式学习数据的联合分布
   $$
   P(X,Y) = P(X|Y)P(Y)
   $$
    然后通过最大后验概率确定数据的类别，后验概率计算根据贝叶斯定理进行：
   $$
   \begin{align}
   P(Y=c_k|X=x) &= \frac{P(X=x,Y = c_k)}{P(X = x)} = \frac{P(X = x|Y=c_k)P(Y=c_k)}{\sum_k P(X = x|Y=c_k)P(Y=c_k)} \\\\
   &\propto {P(X = x|Y=c_k)P(Y=c_k)}
   \end{align}
   $$
   为简化计算，对上述条件概率做条件独立性假设，最终朴素贝叶斯分类器可以表示成：
   $$
   \begin{align}
   y = f(x) &= \arg \max_{c_k} P(X = x|Y=c_k)P(Y=c_k) \\\\
   & = P(Y=c_k)\prod_i P(X^{(i)} = x^{(i)}|Y=c_k)
   \end{align}
   $$
   **注意**：

   1. 朴素贝叶斯方法首先学习数据的联合概率分布，所以其属于生成模型
   2. 通过设置0-1损失函数，朴素贝叶斯方法中的后验概率最大化等价于期望风险最小化。

   朴素贝叶斯方法中参数条件概率(**先验概率**)可以通过**最大似然进行估计**，即
   $$
   \begin{align}
   P(Y=c_k) &= \frac{\sum _{i=1}^n\Bbb I[y_i = c_k]}{n} \\\\
   P(X^{(j)} = a_{jl}|Y=c_k) &=  \frac{\sum _{i=1}^n\Bbb I[y_i = c_k, x_i^{(j)} = a_{jl}]}{\sum _{i=1}^n\Bbb I[y_i = c_k]}
   \end{align}
   $$
   利用极大似然估计可能出现所要估计的值为0的情况，一般采用贝叶斯估计来解决此问题（$K$为总类别数）。
   $$
   \begin{align}
   P_\lambda (Y=c_k) &= \frac{\sum _{i=1}^n\Bbb I[y_i = c_k] + \lambda}{n + K\lambda}\\\\
   P_\lambda (X^{(j)} = a_{jl}|Y=c_k) &=  \frac{\sum _{i=1}^n\Bbb I[y_i = c_k, x_i^{(j)}= a_{jl}] + \lambda}{\sum _{i=1}^n\Bbb I[y_i = c_k] + S_j\lambda} 
   \end{align}
   $$
   上述公式中$\lambda>0$，当$\lambda=1$时又称拉普拉斯平滑（Laplace Smoothing）

4. ### 决策树

   $D$ 样本集合、类别数为$K$

   决策树(Decision Tree)是一种基本的分类与回归方法。常用决策树算法有 ID3、C4.5、CART。

   决策树的关键是如何选择每次划分的特征，一般采用启发式方法。常见启发式函数有:

   #### 1. ID3——最大信息增益：
   决策前经验信息熵：
   $$
   H(D) = -\sum_{k=1}^K\frac{|C_k|}{|D|}\log\frac{|C_k|}{|D|}
   $$
   $C_k$ 表示D中第$k$类样本子集。
   特征A对数据集$D$的信息熵 H(D|A)为：
   $$
   H(D|A) = \sum_{i=1}^{A_l}\frac{|D_i|}{|D|}H(D_i) =  \sum_{i=1}^{A_l}\frac{|D_i|}{|D|}(-\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}\log\frac{|D_{ik}|}{|D_i|})
   $$
   $A_l$为特征A取值个数，$D_i$ 表示特征A 取第$i$ 个值的样本子集。$D_{ik}$ 表示$D_i$ 中属于$k$ 类中样本子集。
   信息增益：
   $$
   G(D,A) = H(D) - H(D|A)
   $$

   #### 2. C4.5——最大信息增益比
   特征A 对数据集D 的信息增益比定义为：
   $$
   G_R(D,A) = \frac{G(D,A)}{H_A(D)}
   $$

   其中$H_A(D)= -\sum_{i=1}^{A_l}\frac{|D_i|}{|D|}\log\frac{|D_i|}{|D|}$，称为 数据集D关于A的取值熵。

   #### 3. CART——基尼指数（Gini）
   分类与回归树CART (Classification and Regression Tree)。在回归问题中，采用平方误差最小化的准则进行特征选择，而在分类问题中采用最小化基尼指数的原则进行特征选择。
   Gini 描述的是数据的纯度，与信息熵含义类似：
   $$
   \mathrm {Gini}(D) = 1 - \sum_{i=1}^K{(\frac{|C_k|}{|D|})}^2
   $$
   特征A 对应的Gini指数对应为：
   $$
   \mathrm{Gini}(D|A) = \sum_{i=1}^{A_l}\frac{|D_i|}{|D|}\mathrm{Gini}(D_i)
   $$

   CART是一颗二叉树，采用二元切分法，因此上述公式中只对两项进行求和。CART在每次迭代过程中选Gini指数最小的特征及其对应的特征值作为切分点进行分类。

5. ###  线性回归

   $T = \{(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\},x_i\in \Bbb R^D,y\in \Bbb R$

   线性回归问题通过上述数据训练寻找合适的线性预测函数$f(x) = w^\mathrm Tx + b$来预测样本$x$的$y$值。线性回归通过最小化如下平方和误差来求解$w, b$:
   $$
   \min_{w,b} L(Y,F(x)) = \frac{1}{2}\sum_{i=1}^n(f(x_i)-y_i)^2 = \frac{1}{2}\sum_{i=1}^n(w^\mathrm Tx_i+b-y_i)^2 = \frac{1}{2}\|w^\mathrm TX + b - Y\|^2
   $$
   上述优化问题存在解析解.令$\bar w = (w^\mathrm T,b)^\mathrm T, \bar X = (X^\mathrm T,1)^\mathrm T$,则解析为$\bar w = (\bar X\bar X^\mathrm T)^{-1}\bar XY^\mathrm T$

   **广义线性模型**，假设$g$为单调递增可微连续函数，$y =g^{-1} (w^\mathrm Tx + b)$ 为广义线性模型。

6. ### 主成分分析

   样本矩阵$X=[x_1,x_2,\cdots,x_n]$，其中每个样本$x_i \in \mathrm R^m$.

   样本均值向量记为$\bar x = \frac{1}{n}\sum_{i=1}^n x_i$

   样本矩阵中心化后的矩阵仍记为$X$, 则样本协方差矩阵$S =\frac{1}{n-1}XX^T$ (注：机器学习领域一般写成$\frac{1}{n}XX^T$).

   $S = \frac{1}{n-1}\sum_{i=1}^n x_i x_i^T$(样本$x_i$中心化后仍记为$x_i$)

   对矩阵$S$进行特征分解，其前$d$最大特征值$\lambda_i$所对应的特征向量为$v_i$ . $V = [v_1,v_2,\cdots, v_d,]$,则样本的主成分为$Y = V^TX$.

   也可以采用奇异值分解求取主成分。记$X' = \frac{1}{\sqrt{n-1}}X$,其奇异值分解为$X'=U\Sigma V^T$.则样本主成分为$Y=U_d^TX$.

7. ### 线性判别分析

8. ### 流形学习

   - 多维缩放(MDS):
   - 等距特征映射(ISOMAP)
   - 局部线性嵌入(LLE, Locally Linear Embedding)
   - 局部切空间对齐(LTSA, Local tangent Space Alignment)
   - 拉普拉斯特征映射(LE, Laplacian Eigenmaps)

9. ###  逻辑斯蒂回归

   逻辑斯蒂回归(Logistic Regression)解决的是分类问题。令$\mu = p(y=1|x)$即样本属于正例的概率，则$1-\mu$表示样本属于负类的概率。LR认为对数几率(log odds)或logit函数是变量$x$的线性函数（广义线性模型），即：
$$
   \mathrm{logit}(\mu) = \log \frac{\mu}{1-\mu} = w^\mathrm{T}x+b
$$
   从而:
$$
   \mu = \frac{1}{1+\exp {(-(w^\mathrm{T}x+b)})}
$$
   不是一般性，记$w=(w,b), x=(x,1)$。一般应用最大似然法来估计上述模型的参数。设$P(Y=1|x）= \pi(x), P(Y=0|x) = 1-\pi(x)$, 则训练数据的似然函数为:
$$
   \prod_{i=1}^n\pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i}
$$
   对数似然函数为：
$$
   \begin{align}L(w) &= \sum_{i=1}^n[y_i\log\pi(x_i) + (1-y_i)\log(1-\pi(x_i))] \\\\
   &= \sum_{i=1}^n[y_i\log\frac{\pi(x_i)}{1-\pi(x_i)} + \log(1-\pi(x_i))]\\\\
   &= \sum_{i=1}^n[y_i(w\cdot x_i) - \log (1+\exp(w\cdot x_i)) 
   \end{align}
$$
   对$L(w)$求极大值得到$w$的估计，可以采用梯度下降法或者拟牛顿法。

   #### 注意：

   - 最大化上述对数似然函数，等价于最小化对数损失函数：

$$
   \begin{align}
   L(w,x,y) &= -\log P(y|x) = 
   \begin{cases}
   -\log P(y=1|x) = -y\log\pi(x), &y=1\\\\
   -\log P(y=0|x)  = -(1-y)\log(1-\pi(x)), & y=0
   \end{cases}
    \\\\
    &= y\log\pi(x) + (1-y)\log(1-\pi(x))
   \end{align}
$$

   - 最大化上述对数似然函数，等价于最小化标签的真实分布和标签的条件分布之间的**KL散度**。
     $$
     KL(y,p(y|x)) = \sum_y ylogP(y|x) = y\log\pi(x) + (1-y)\log(1-\pi(x))
     $$

   逻辑斯蒂回归是二类分类模型，可将其推广到多项逻辑斯蒂回归模型。假设离散类型变量$Y$取值为集合$\{1,2,\cdots,K\}$，那么多项逻辑斯蒂回归模型的为：
$$
   \begin{align}P(Y=k|x) &= \frac{\exp(w_k\cdot x)}{1+\sum_{i=1}^{K-1}\exp(w_k\cdot x)} \\\\
   P(Y=K|x) &= \frac{1}{1+\sum_{i=1}^{K-1}\exp(w_k\cdot x)}
   \end{align}
$$
   然后通过最大化对数似然函数或者最小化对数损失函数、最小化KL散度等方法对模型参数进行估计。

11. ###  支持向量机

    两向量$u,v$夹角$\alpha$，则其内积$u\cdot v = \|u\|\|v\|\cos\alpha$

    不在同一直线的的三点$u, v, w$，点$w$到直线$\vec {uv}$的距离公式为：
    $$
    \left\|w-u - \frac{(w-u)\cdot (v-u)}{\|w-u\|\|v-u\|}\|(w-u)\|\frac{v-u}{\|v-u\|}\right\|= \|w-u - \frac{(w-u)\cdot (v-u)}{\|v-u\|^2}(v-u)\|
    $$


    ![点到直线的距离](.\images\distance.jpg"点到直线的距离")
    
    支持向量机的决策函数$f(x) = sign(w\cdot x +b)$
    
    分类超平面：$w\cdot x + b = 0$ 
    
    点$x$到上述分类超平面的距离：$\frac{\|w\cdot x + b\|} {\|w\|}$
    
    样本点$(x,y)$到分类超平面的**函数间隔**：$\hat \gamma = y(w\cdot x + b)$
    
    样本点$(x,y)$到分类超平面的**几何间隔**：$\gamma =  y(w\cdot x + b)/\|w\|$
    
    线性支持向量机的目标是在保证分类正确的前提最大化点到分类超平面的几何间隔。
    $$
    \begin{align}\min _{w,b}&: \frac{1}{2}\|w\|^2 \\\\
    s.t&:y_i(w\cdot x_i + b) \ge 1, i=1,2,\cdots,n
    \end{align}
    $$
    该问题是一个凸二次规划，利用拉格朗日乘子法通过对其对偶问题就行求解，拉格朗日函数：
    $$
    L(w,b,\alpha)= \frac{1}{2}\|w\|^2 + \sum_i\alpha_i(1-y_i(w\cdot x_i + b)) \tag{1}
    $$
    通过对$w, b $求导，令导数为零可得：
    $$
    \begin{align}w &= \sum_i \alpha_iy_ix_i \\\\
    \sum_i \alpha_iy_i &=0
    \end{align}
    $$
    将上式带入到拉格朗日函数(1)中，得到如下对偶优化问题：
    $$
    \begin{align}
    \max_{\alpha}&: L(\alpha) = \sum_i \alpha_i - \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_iy_iy_jx_ix_j\\\\
    s.t&: \sum_i \alpha_iy_i =0,  i=1,2,\cdots,n
    \end{align}
    $$
    求出$\alpha$后，依次可求得$w, b$ ,最后得到分类函数为：
    $$
    f(x) = \sum_i\alpha_iy_i(x_i\cdot x) + b
    $$
    在上述拉格朗日求解过程中KKT条件为$(i=1,2,\cdots,n)$：
    $$
    \begin{align}
    \alpha_i &\ge 0 \\\\
    y_i(w\cdot x_i + b) &\ge 1, \\\\
    \alpha_i(y_i(w\cdot x_i + b)-1) &= 0 \\\\
    \end{align}
    $$
    上述公式中第三个等式被称为松弛条件。由上述条件可知：
    
    - 当$\alpha_i>0$时，则$y_i(w\cdot x_i + b)=1 $，此时样本点$(x_i,y_i)$落在分类间隔的边界上，我们称其为支持向量。
    
    - 当$\alpha_i=0$，此时所对应的的样本$(x_i,y_i)$不出现在$w$求解表达式中，对支持向量机决策边界不起任何作用。
    
      所以此方法被称为支持向量机，是因为决策函数仅有少数训练样本决定(支持向量)。
    
    软间隔最大化支持向量机，求解如下优化问题($\xi$称为松弛变量)：
    $$
    \begin{align}
    \min_{w,b,\xi}:&\frac{1}{2}\|w\|^2 + C\sum_i \xi_i\\\\
    s.t:& y_i(w\cdot x_i+b) \ge 1- \xi_i, i=1,2,\cdots,n\\\\
    & \xi_i \ge 0, i=1,2,\cdots,n
    \end{align}
    $$
    其对偶优化问题为：
    $$
    \begin{align}\min_\alpha:& \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) -\sum_i\alpha_i\\\\
    s.t:&\sum_i\alpha_iy_i=0 \\\\
    &0\le\alpha_i\le C, i=1,2,\cdots,n
    \end{align}
    $$
    软间隔支持向量机的支持向量$x_i$所对应的的$\alpha_i>0$，他们或者在间隔边界上，或者在间隔边界与分离超平面之间，或者在分离超平面误分一侧。
    
    - 若$0<\alpha_i < C$，则$\xi_i =0$，样本$x_i$恰好落在间隔边界上；
    - $\alpha_i = C$，且$0<\xi_i<1$，则样本$x_i$被正确分类，且落在间隔边界和分离超平面之间。
    - $\alpha_i = C$，且$\xi_i=1$，则样本$x_i$落在分离超平面上
    - $\alpha_i = C$，且$\xi_i>1$，则样本$x_i$落在分离超平面误分类一侧。
    
    **注意**：若使用合页损失函数$L(z)= \max(0,1-z)$,则软间隔支持向量机等价于最小化正则化的合页损失函数：
    $$
    \min_{w,b}:\sum_i  L(1-y_i(w\cdot x_i+b))+\lambda\|w\|^2
    $$
    支持向量机中常使用的核函数：
    
    - 线性核：$\kappa(x_i,x_j) = x_i^\mathrm Tx_j$
    - 多项式核：$\kappa(x_i,x_j) = (x_i^\mathrm Tx_j)^d$，其中$d$为多项式的次数
    - 高斯核(径向基核)：$\kappa(x_i,x_j) = \exp\left(-\frac{\|x_i - x_j\|^2}{\sigma^2} \right)$，其中$\sigma$为高斯核的带宽(width)。
    - 拉普拉斯核：$\kappa(x_i,x_j) = \exp\left(-\frac{\|x_i - x_j\|}{\sigma} \right)$，其中$\sigma>0$。
    - Sigmoid核：$\kappa(x_i,x_j) = \tanh (\beta x_i^\mathrm Tx_j + \alpha)$，其中$\beta>0, \alpha<0$。

12. ### 支持向量回归

    不同于支持向量机，支持向量回归希望学习一个回归模型。而一般传统的回归方法认为只有当预测值和真实值相等时，损失才为0，支持向量回归允许预测值和真实值之前存在小于$\epsilon$ 的误差，即只有当预测值和真实值误差绝对值大于$\epsilon$才计算误差。

    ![持向量回归](.\images\SVR.png"支持向量回归示意图")

    其所解决的优化问题为：
    $$
    \begin{align}
    \min_{w,b}: \frac{1}{2}\|w\|^2 + C\sum_i \cal L_\epsilon(w*x_i+b-y_i)
    \end{align}
    $$
    其中损失函数定义如下：
    $$
    \cal L_\epsilon(Z) = \left \{
    \begin{align}
    &0, &|Z|\le \epsilon\\\\
    &|Z|-\epsilon, & |Z|> \epsilon
    \end{align} 
    \right.
    $$
    引入松弛变量$\xi_i$和$\hat \xi_i$后，上述支持向量回归的最优化问题可以写成如下形式： 
    $$
    \begin{align}
    \min_{w,b}: &\frac{1}{2}\|w\|^2 + C\sum_i(\xi_i+\hat\xi_i)\\\\
    s.t:&w*x_i+b-y_i\le\epsilon + \xi_i, \\\\
    &y_i-w*x_i-b\le\epsilon + \hat\xi_i, \\\\
    &\xi_i  \ge0,\hat\xi_i\ge0, i=1,2,\cdots,n
    \end{align}
    $$
     上述问题的KKT条件为：
    $$
    \left\{\begin{align}
    \alpha_i(w*x_i+b-y_i-\epsilon-\xi_i) &= 0 \\\\
    \hat\alpha_i(y_i - w*x_i-b-\epsilon-\hat\xi_i) &= 0 \\\\
    \alpha_i\hat\alpha_i&=0 \\\\
    \xi_i\hat\xi_i&=0\\\\
    (C-\alpha_i)\xi_i&=0\\\\
    (C-\hat\alpha_i)\hat\xi_i&=0
    \end{align}
    \right.
    $$
    隔离带里边的点(不包括边界)，其满足$w*x_i+b-y_i-\epsilon-\xi_i \ne 0$ 或者$y_i - w*x_i-b-\epsilon-\hat\xi_i \ne0$。因此其对应$\alpha_i$或$\hat\alpha_i$均为0，隔离带外边(包括边界)的点满足$w*x_i+b-y_i-\epsilon-\xi_i = 0$ 或者$y_i - w*x_i-b-\epsilon-\hat\xi_i=0$。此时由KKT条件可知，其对应$\alpha_i$或$\hat\alpha_i$不为0，为支持向量。另外$\alpha_i$或$\hat\alpha_i$必有一个为0，$\xi_i% $和$\hat\xi_i$必有一个为0。

    支持向量最后的解的形式为:
    $$
    f(x) = \sum_i (\hat\alpha_i-\alpha_i)x_i^\mathrm Tx+b
    $$

13. ###  期望最大化(EM)算法

    概率模型中，有时候既有观测变量(observable variable)，又有隐变量或者潜在变量(latent variable).

    观测数据$Y=(Y_1,Y_2,\cdots,Y_n)^\mathrm T$，未观测数据$Z=(Z_1,Z_2\cdots,Z_n)^\mathrm T$。

    考虑利用最大似然估计求解模型参数:
    $$
    \max_\theta:\log P(Y|\theta) = \log \left(\sum_Z P(Y, Z|\theta)\right) = \log \left(\sum_Z P(Y|Z,\theta)P(Z|\theta)\right)
    $$
    由于上式对数中存在求和表达式，增加了求解困难。

    EM算法最后通过迭代求解，在第$i+1$步通过求解如下优化问题：
    $$
    \max_\theta: \sum_Z P(Z|Y,\theta^{(i)})\log P(Y,Z|\theta) \tag{1}
    $$
    其中$P(Z|Y,\theta^{(i)})$为用上步迭代值$\theta^{(i)}$和观测数据求出的隐变量$Z$的概率分布。

    推导：
    $$
    \begin{align}
    \log \left(\sum_Z P(Y, Z|\theta)\right) &= \log \sum_ZP(Z|Y,\theta^{(i)})\frac{P(Y,Z|\theta)}{P(Z|Y,\theta^{(i)})}   \\\\
    &\ge \sum_ZP(Z|Y,\theta^{(i)})\log \frac{P(Y,Z|\theta)}{P(Z|Y,\theta^{(i)})} \tag{Jensen不等式} 
    \end{align}
    $$
    由于$P(Z|Y,\theta^{(i)})$为常数，最大化右边等价于最大化$\sum_ZP(Z|Y,\theta^{(i)})\log P(Y,Z|\theta)$,也即公式(1).

    公式(1)又被称为**$Q$函数**，它是完全数据的对数似然函数$P(Y,Z|\theta)$关于在给定观测数据$Y$和当前参数$\theta ^{(i)}$下未观测数据$Z$的条件概率分布$P(Z|Y,\theta^{(i)})$的期望，即：

    $Q(\theta, \theta^{(i)})= \mathrm E_Z[\log P(Y,Z|\theta)|Y,\theta^{(i)}] = \sum_Z P(Z|Y,\theta^{(i)})\log P(Y,Z|\theta)  \tag{2}$

    要注意，在迭代求解公式(1)时，有时候并不需要真正求出$P(Z|Y,\theta^{(i)})$，而是在求和过程中会得到关于$P(Z|Y,\theta^{(i)})$的期望表达式，因此我们只需根据之前的参数$\theta^{(i)}$和已知数据，求出隐变量$Z$的期望。这种技巧在求解高斯混合模型、投硬币模型中经常用到。

    ![EM优化](.\images\EM.jpg"EM算法优化示意图")

    EM算法步骤：

    1. 选择参数初始值$\theta^{(0)}$,开始迭代;
    2. E步骤：按照公式(2),利用上步迭代值$\theta ^{(i)}$计算完全数据的条件期望(Q函数)。
    3. M步骤：最大化Q函数，求得$\theta ^{(i+1)}$
    4. 重复2,3步，直至手链或者最大迭代次数。

14. ### 集成学习(提升方法)

    ####  1、AdaBoost算法

    对于二分类问题，通过组合多个弱分类器（仅比随机猜测好一点），构成一个分类正确率比较高的分类器。AdaBoost算法通过提高被前一轮分类器误分类样本的权值，降低被正确分类样本的权值来改变训练数据的概率分布。设$D_m=(w_{m1},w_{m2},\cdots,w_{mN})$为第$m$个分类器训练数据的权重，初始时每个样本的权重相等，即$D_1=(w_{11},w_{12},\cdots,w_{1N}) =(\frac{1}{N},\frac{1}{N},\cdots,\frac{1}{N})$。设$G_m: \cal X \rightarrow \{-1,1\}$为第$m$个弱分类器，其分类误差率为$e_m=\sum_{i=1}^NP(G(x_i)\neq y_i)=\sum_{i=1}^Nw_{mi}\Bbb I[G(x_i)\neq y_i]$。分类器$G_m$的组合权重为：
    $$
    \alpha_m = \frac{1}{2}\ln\frac{1-e_m}{e_m}
    $$
    下一轮迭代时，其样本权重$D_{m+1}$通过如下公式计算：
    $$
    w_{m+1,i} = \frac{w_{mi}}{Z_{m+1}}\exp(-\alpha_mG(x_i)y_i)
    $$
    其中$Z_{m+1}$是规范化系数，确保$D_{m+1}$是一个概率分布。

    最终的分类器是所有弱分类器的加权线性组合。
    $$
    f(x) = sign\left(\sum_{i=1}^M\alpha_iG_i(x)\right)
    $$
    AdaBoost算法的最终训练误差界为$\prod_{i=1}^MZ_i$。另外AdaBoost算法的训练误差是以**指数速率**下降滴。这可以保证AdaBoost算法能很快收敛。

    ​	        AdaBoost算法是一种加法模型，它以指数函数为损失函数，以前向分步算法为学习算法的二分类学习方法。从偏差-方差分解的角度看，AdaBoost算法主要通过降低偏差来提高性能。

    #### 2、提升树

    ​		提升树是以CART决策树为基本分类器的提升方法。提升树模型可以表示成如下CART树的加法模型：
    $$
    f_M(x) = \sum_{m=1}^MT(x;\Theta_m)
    $$
    其中$T(x;\Theta_m)$表示CART树，$\Theta_m$为决策树的参数,$M$为决策树的个数。

    ​     	首先假设$f_0(x)=0$，则第$m$步的模型是
    $$
    f_m(x) = f_{m-1} + T(x;\Theta_m)
    $$
    其通过最小化经验风险来确定模型参数$\Theta_m$。
    $$
    \hat \Theta_m = \arg \min_{\theta_m}\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\Theta_m))
    $$
    ​		对于二分类问题，只需要基分类器限制为二分类分类树，然后采用AdaBoost算法。对于回归问题，若采用平方误差损失时，上述公式中，损失函数变为：
    $$
    \begin{align}
    \sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\Theta_m)) &= \sum_{i=1}^N [y_i - f_{m-1}(x_i) - T(x_i;\Theta_m)]^2 \\
    & = \sum_{i=1}^N (r_i - T(x_i;\Theta_m))^2
    \end{align}
    $$
    $r_i$为上一决策时拟合残差，因此当前决策树只需要拟合前一决策树的残差即可。

    **梯度提升树（GBDT， Gradient Boosting Decision Tree）**：当损失函数为指数损失或者平方差损失时，提升树的每一步优化比较简单，当损失函数为一般损失函数时，采用梯度提升方法算法。平方损失时每一步的优化时拟合上一步的拟合误差，而梯度提升方法则拟合负梯度在上一个模型位置上的值，即
    $$
    -\left[\frac{\partial L(x_i,f(x_i))}{\partial f(x_i)}\right]_{f_{m-1}(x_i)}
    $$

    #### 3、Bagging(Boostratp AGGregatING)

    ​        Bagging算法的基本思想是通过自助采样法(boostrap sampling)，采样出$T$个含$m$个训练样本的训练数据集，在这多个训练集上训练出多个弱分类器，然后对于分类问题，采用委员会投票得到最后的分类结果，对于回归为题，一般采用多分类器输出的平均值作为最后的输出。

    ​         自助一般采用有放回采样，因此每个分类器只使用了初始训练集中约63.2%的样本，剩余约36.8%的样本可以用作验证机来对分类器的泛化性能进行“**包外估计**”(out-of-bag estimate)。分类器包外估计的误差可以用来衡量该分类器的可信度。

    ​          从偏差-方差分解的角度看，Bagging算法主要通过降低方差来提高性能。

    #### 4、随机森林

    ​		随机森林本质上是Bagging的一个扩展变体。随机森林以决策树为基学习器，在自助采样获取训练数据集的基础上，训练过程中引入了随机属性选择。具体来说在传统决策树算法中，当前结点会从当前属性集合(假设有$d$个属性)中选择一个最优属性来进行判断扩展；而随机森林则是先从该属性集合中随机选择$k(k<d)$个属性，然后从这$k$个属性中选择最优属性进行判断扩展。

    #### 5、XGBoost

    XGBoost(Extreme Gradient Boosting Tree)可以看做是GBDT的扩展。不同于GBDT, XGBoost为了防止过拟和，在优化目标中加入限制模型复杂度的正则化项，其具体优化目标如下：
    $$
    obj = \sum_{i=1}^mL(y_i,f(x_i)) + \Omega(f(x_i))
    $$
    具体的在第$t$次迭代过程中，其优化目标为：
    $$
    obj^{(t)} = \sum_{i=1}^mL(y_i,f_{(t-1)}(x_i)+ h_t(x_i)) + \gamma J_t + \frac{1}{2}\lambda\sum_{i=1}^{J_t}w^2_{hi}
    $$
    这里$J_t$是第$t$次迭代树$h_t$的叶子节点数目，$w_{ti}$为叶子节点的取值。在构建树$h_t$过程中，需要解决两个优化问题，一个是确定最优的$J_t$个叶子节点也即划分区域，二是确定每个叶子节点的最优取值$w_{ti}$。

    不同于BGDT在优化损失函数时，模拟损失函数的负梯度，即用损失函数一阶泰勒展开近似，XGBoost对优化目标损失函数部分采用二阶泰勒展开近似，即：
    $$
    \begin{align}
    obj^{(t)} &= \sum_{i=1}^m\left(L(y_i,f_{(t-1)}(x_i)) + \frac{\partial L(y_i,f_{(t-1)}(x_i))}{\partial f_{(t-1)}(x_i)}h_t(x_i) + \frac{1}{2}\frac{\partial^2 L(y_i,f_{(t-1)}(x_i))}{\partial f^2_{(t-1)}(x_i)}h^2_t(x_i))\right) + \gamma J_t + \frac{1}{2}\lambda\sum_{i=1}^{J_t}w^2_{hi} \\
    &= \sum_{i=1}^m \left(L(y_i,f_{(t-1)}(x_i)) + g_{ti}h_t(x_i) + \frac{1}{2}h_{ti}h^2_t(x_i)\right) +  \gamma J_t + \frac{1}{2}\lambda\sum_{i=1}^{J_t}w^2_{hi} 
    \end{align}
    $$
    由于$L(y_i,f_{(t-1)}(x_i))$是常数，且每个叶子节点取值唯一，所以上述目标函数可以进一步优化：
    $$
    \begin{align}
    obj^{(t)} &\approx \sum_{i=1}^m \left(g_{ti}h_t(x_i) + \frac{1}{2}h_{ti}h^2_t(x_i)\right) +  \gamma J_t + \frac{1}{2}\lambda\sum_{i=1}^{J_t}w^2_{hi}  \\
    & = \sum_{i=1}^{J_t}\sum_{x_j\in R_{ti}}g_{tj}w_{ti} + \frac{1}{2}\sum_{i=1}^{J_t}\sum_{x_j\in R_{ti}}h_{tj}w^2_{ti}  + \gamma J_t + \frac{1}{2}\lambda\sum_{i=1}^{J_t}w^2_{hi} \\
    & = \frac{1}{2}\sum_{i=1}^{J_t}\left(\sum_{x_j\in R_{ti}}h_{tj}+\lambda\right)w^2_{ti} + \sum_{i=1}^{J_t}\sum_{x_j\in R_{ti}}g_{tj}w_{ti} + \gamma J_t \\
    &= \frac{1}{2}\sum_{i=1}^{J_t}(H_{hi}+\lambda)w^2_{hi}  + \sum_{i=1}^{J_t}G_{hi}w_{hi} + \gamma J_t
    \end{align}
    $$
    可以看出上式为关于$w_{hi}$的二次函数。所以在$h_t$树结构确定的情况下，每个叶子节点的最优取值为
    $$
    w_{hi} = -\frac{G_{hi}}{H_{hi}+\lambda}
    $$
    这就解决了树$h_t$的叶子节点的最优取值问题。这样优化目标的值为
    $$
    obj^{(t)} = -\frac{1}{2}\sum_{i=1}^{J_t}\frac{G^2_{hi}}{H_{ti}+ \lambda} + \gamma J_t
    $$
    对于树$h_t$对当前节点如何选择最优属性和最优划分值，XGBoost和GBDT一样，采用贪心策略，通过遍历所有特征和其所有划分值，寻找使$obj^{(t)}$最小的特征及其划分值。在划分节点是选择能最小化优化目标$obj^{(t)}$的属性和其划分值。

15. ### 

    

16. ###  神经网络

17. ###  最大熵模型

18. ###  隐马尔可夫模型

19. ###  条件随机场







