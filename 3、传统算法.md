# 传统算法

 训练数据集$T = \{(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\},x_i\in \Bbb R^D,y\in\{+1,-1\}$

1. ###  感知机

   给定上述训练数据集，感知机算法寻找如下决策函数$y = f(x) = sign (wx+b)$.

   原始感知机算法($\eta$为学习率)：

   ***

   1. 初始化：$w, b$

   2. 在训练集中寻找误分类样本$(x_i,y_i)$,即$y_i(wx_i+b_i) \le 0 $

      更新$w,b$:
      $$
      \begin{align}
      w &= w + \eta y_ix_i\\\\
      b &= b + \eta y_i
      \end{align}
      $$

   3. 重复步骤（2），直到没有误分类样本，终止。

   ***

   一般学习问题对偶形式的基本思想是将待学习参数$w, b$表示成训练样本$x_i$和标记$y_i$的线性组合形式，通过求解其组合系数求得$w, b$。在上述原始感知机学习算法中，初始化时，若设$w=0,b=0$，则最后学习得到的$w, b$具有如下形式：
   $$
   \begin{align}
   w &= \sum_{i=1}^n\alpha_iy_ix_i\\\\
   b &= \sum_{i=1}^n \alpha_iy_i
   \end{align}
   $$
   上述表示模式称为感知机学习算法的对偶模式。

   感知机学习算法的对偶形式：

   ***

   输出：$\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_n)$；感知机模型为$f(x) =sign(\sum\limits_{i=1}^n\alpha_iy_ix_i\cdot x + \sum\limits_{i=1}^n\alpha_iy_i)$

   1. 初始化 $\alpha = 0$

   2. 训练集中选取误分类样本$(x_j,y_j)$即$y_j(\sum\limits_{i=1}^n\alpha_iy_ix_i\cdot x_j + \sum\limits_{i=1}^n\alpha_iy_i)\le 0$
      $$
      \alpha_i =\alpha_i + \eta
      $$

   3. 

   ***

2. ###  线性回归

   $T = \{(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\},x_i\in \Bbb R^D,y\in \Bbb R$

   线性回归问题通过上述数据训练寻找合适的线性预测函数$f(x) = w^\mathrm Tx + b$来预测样本$x$的$y$值。线性回归通过最小化如下平方和误差来求解$w, b$:
   $$
   \min_{w,b} L(Y,F(x)) = \frac{1}{2}\sum_{i=1}^n(f(x_i)-y_i)^2 = \frac{1}{2}\sum_{i=1}^n(w^\mathrm Tx_i+b-y_i)^2 = \frac{1}{2}\|w^\mathrm TX + b - Y\|^2
   $$
   上述优化问题存在解析解.令$\bar w = (w^\mathrm T,b)^\mathrm T, \bar X = (X^\mathrm T,1)^\mathrm T$,则解析为$\bar w = (\bar X\bar X^\mathrm T)^{-1}\bar XY^\mathrm T$

3. ### 主成分分析

4. ### 线性判别分析

5. ### 流形学习

6. ###  决策树

7. ###  $k$近邻算法

8. ###  逻辑斯蒂回归

9. ###  支持向量机

10. ###  期望最大化(EM)算法

11. ###  贝叶斯估计

12. ###  集成学习

13. ###  神经网络

14. ###  最大熵模型

15. ###  隐马尔可夫模型

16. ###  条件随机场







