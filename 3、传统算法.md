# 传统算法

 训练数据集$T = \{(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\},x_i\in \Bbb R^D,y\in\{+1,-1\}$

1. ###  感知机

   给定上述训练数据集，感知机算法寻找如下决策函数$y = f(x) = sign (wx+b)$.

   原始感知机算法($\eta$为学习率)：

   ***

   1. 初始化：$w, b$

   2. 在训练集中寻找误分类样本$(x_i,y_i)$,即$y_i(wx_i+b_i) \le 0 $

      更新$w,b$:
      $$
      \begin{align}
      w &= w + \eta y_ix_i\\\\
      b &= b + \eta y_i
      \end{align}
      $$

   3. 重复步骤（2），直到没有误分类样本，终止。

   ***

   一般学习问题对偶形式的基本思想是将待学习参数$w, b$表示成训练样本$x_i$和标记$y_i$的线性组合形式，通过求解其组合系数求得$w, b$。在上述原始感知机学习算法中，初始化时，若设$w=0,b=0$，则最后学习得到的$w, b$具有如下形式：
   $$
   \begin{align}
   w &= \sum_{i=1}^n\alpha_iy_ix_i\\\\
   b &= \sum_{i=1}^n \alpha_iy_i
   \end{align}
   $$
   上述表示模式称为感知机学习算法的对偶模式。

   感知机学习算法的对偶形式：

   ***

   输出：$\alpha=(\alpha_1,\alpha_2,\cdots,\alpha_n)$；感知机模型为$f(x) =sign(\sum\limits_{i=1}^n\alpha_iy_ix_i\cdot x + \sum\limits_{i=1}^n\alpha_iy_i)$

   1. 初始化 $\alpha = 0$

   2. 训练集中选取误分类样本$(x_j,y_j)$即$y_j(\sum\limits_{i=1}^n\alpha_iy_ix_i\cdot x_j + \sum\limits_{i=1}^n\alpha_iy_i)\le 0$
      $$
      \alpha_j =\alpha_j + \eta
      $$

   3. 重复步骤（2），直到没有误分类样本，终止。

   ***

2. ### $k$近邻算法

   $k$近邻分类决策规则是多数表决(majority voting rule)，即由输入实例的$k$个近邻训练实例的多数类决定输入实例的类别。这其实是一种经验风险最小化的决策规则。假设分类损失函数为0-1损失函数，$k$近邻分类函数为$f: \Bbb R^n \rightarrow \{c_1,c_2,\cdots,c_K\}$

   误分类概率为：
   $$
   P(Y \ne f(X)) = 1 - P(Y = f(X))
   $$
   给定实例$x\in \cal X$,其$k$近邻训练实例集合记为$\cal N_k(x)$，设涵盖$\cal N_k(x)$区域的类别为$c_j$($x$的真实类别)，则误分类概率为：
   $$
   \frac{1}{k}\sum_{x_i\in \cal N_k(x)}\Bbb I(y_i \ne c_j) = 1 - \frac{1}{k}\sum_{x_i\in \cal N_k(x)}\Bbb I(y_i = c_j)
   $$
   要使误分类概率即经验风险最小，就要使$\frac{1}{k}\sum_{x_i\in \cal N_k(x)}\Bbb I(y_i = c_j)$最大，所以多数投票规则等于经验风险最小化。

   实现$k$近邻算法的关键是如何对训练数据进行快速的近邻检索。

   **关键点**：1、构造平衡kd树来实现。2、基于kd树的近邻检索。

3. ### 朴素贝叶斯分类

   朴素贝叶斯方法通过如下公式学习数据的联合分布
   $$
   P(X,Y) = P(X|Y)P(Y)
   $$
    然后通过最大后验概率确定数据的类别，后验概率计算根据贝叶斯定理进行：
   $$
   \begin{align}
   P(Y=c_k|X=x) &= \frac{P(X=x,Y = c_k)}{P(X = x)} = \frac{P(X = x|Y=c_k)P(Y=c_k)}{\sum_k P(X = x|Y=c_k)P(Y=c_k)} \\\\
   &\propto {P(X = x|Y=c_k)P(Y=c_k)}
   \end{align}
   $$
   为简化计算，对上述条件概率做条件独立性假设，最终朴素贝叶斯分类器可以表示成：
   $$
   \begin{align}
   y = f(x) &= \arg \max_{c_k} P(X = x|Y=c_k)P(Y=c_k) \\\\
   & = P(Y=c_k)\prod_i P(X^{(i)} = x^{(i)}|Y=c_k)
   \end{align}
   $$
   **注意**：

   1. 朴素贝叶斯方法首先学习数据的联合概率分布，所以其属于生成模型
   2. 通过设置0-1损失函数，朴素贝叶斯方法中的后验概率最大化等价于期望风险最小化。

   朴素贝叶斯方法中参数条件概率(**先验概率**)可以通过**最大似然进行估计**，即
   $$
   \begin{align}
   P(Y=c_k) &= \frac{\sum _{i=1}^n\Bbb I[y_i = c_k]}{n} \\\\
   P(X^{(j)} = a_{jl}|Y=c_k) &=  \frac{\sum _{i=1}^n\Bbb I[y_i = c_k, x_i^{(j)} = a_{jl}]}{\sum _{i=1}^n\Bbb I[y_i = c_k]}
   \end{align}
   $$
   利用极大似然估计可能出现所要估计的值为0的情况，一般采用贝叶斯估计来解决此问题（$K$为总类别数）。
   $$
   \begin{align}
   P_\lambda (Y=c_k) &= \frac{\sum _{i=1}^n\Bbb I[y_i = c_k] + \lambda}{n + K\lambda}\\\\
   P_\lambda (X^{(j)} = a_{jl}|Y=c_k) &=  \frac{\sum _{i=1}^n\Bbb I[y_i = c_k, x_i^{(j)}= a_{jl}] + \lambda}{\sum _{i=1}^n\Bbb I[y_i = c_k] + S_j\lambda} 
   \end{align}
   $$
   上述公式中$\lambda>0$，当$\lambda=1$时又称拉普拉斯平滑（Laplace Smoothing）

4. ### 决策树

   $D$ 样本集合、类别数为$K$

   决策树(Decision Tree)是一种基本的分类与回归方法。常用决策树算法有 ID3、C4.5、CART。

   决策树的关键是如何选择每次划分的特征，一般采用启发式方法。常见启发式函数有:

   #### 1. ID3——最大信息增益：
   决策前经验信息熵：
   $$
   H(D) = -\sum_{k=1}^K\frac{|C_k|}{|D|}\log\frac{|C_k|}{|D|}
   $$
   $C_k$ 表示D中第$k$类样本子集。
   特征A对数据集$D$的信息熵 H(D|A)为：
   $$
   H(D|A) = \sum_{i=1}^{A_l}\frac{|D_i|}{|D|}H(D_i) =  \sum_{i=1}^{A_l}\frac{|D_i|}{|D|}(-\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}\log\frac{|D_{ik}|}{|D_i|})
   $$
   $A_l$为特征A取值个数，$D_i$ 表示特征A 取第$i$ 个值的样本子集。$D_{ik}$ 表示$D_i$ 中属于$k$ 类中样本子集。
   信息增益：
   $$
   G(D,A) = H(D) - H(D|A)
   $$

   #### 2. C4.5——最大信息增益比
   特征A 对数据集D 的信息增益比定义为：
   $$
   G_R(D,A) = \frac{G(D,A)}{H_A(D)}
   $$

   其中$H_A(D)= -\sum_{i=1}^{A_l}\frac{|D_i|}{|D|}\log\frac{|D_i|}{|D|}$，称为 数据集D关于A的取值熵。

   #### 3. CART——基尼指数（Gini）
   分类与回归树CART (Classification and Regression Tree)。在回归问题中，采用平方误差最小化的准则进行特征选择，而在分类问题中采用最小化基尼指数的原则进行特征选择。
   Gini 描述的是数据的纯度，与信息熵含义类似：
   $$
   \mathrm {Gini}(D) = 1 - \sum_{i=1}^K{(\frac{|C_k|}{|D|})}^2
   $$
   特征A 对应的Gini指数对应为：
   $$
   \mathrm{Gini}(D|A) = \sum_{i=1}^{A_l}\frac{|D_i|}{|D|}\mathrm{Gini}(D_i)
   $$

   CART是一颗二叉树，采用二元切分法，因此上述公式中只对两项进行求和。CART在每次迭代过程中选Gini指数最小的特征及其对应的特征值作为切分点进行分类。

5. ###  线性回归

   $T = \{(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\},x_i\in \Bbb R^D,y\in \Bbb R$

   线性回归问题通过上述数据训练寻找合适的线性预测函数$f(x) = w^\mathrm Tx + b$来预测样本$x$的$y$值。线性回归通过最小化如下平方和误差来求解$w, b$:
   $$
   \min_{w,b} L(Y,F(x)) = \frac{1}{2}\sum_{i=1}^n(f(x_i)-y_i)^2 = \frac{1}{2}\sum_{i=1}^n(w^\mathrm Tx_i+b-y_i)^2 = \frac{1}{2}\|w^\mathrm TX + b - Y\|^2
   $$
   上述优化问题存在解析解.令$\bar w = (w^\mathrm T,b)^\mathrm T, \bar X = (X^\mathrm T,1)^\mathrm T$,则解析为$\bar w = (\bar X\bar X^\mathrm T)^{-1}\bar XY^\mathrm T$

   **广义线性模型**，假设$g$为单调递增可微连续函数，$y =g^{-1} (w^\mathrm Tx + b)$ 为广义线性模型。

6. ### 主成分分析

   样本矩阵$X=[x_1,x_2,\cdots,x_n]$，其中每个样本$x_i \in \mathrm R^m$.

   样本均值向量记为$\bar x = \frac{1}{n}\sum_{i=1}^n x_i$

   样本矩阵中心化后的矩阵仍记为$X$, 则样本协方差矩阵$S =\frac{1}{n-1}XX^T$ (注：机器学习领域一般写成$\frac{1}{n}XX^T$).

   $S = \frac{1}{n-1}\sum_{i=1}^n x_i x_i^T$(样本$x_i$中心化后仍记为$x_i$)

   对矩阵$S$进行特征分解，其前$d$最大特征值$\lambda_i$所对应的特征向量为$v_i$ . $V = [v_1,v_2,\cdots, v_d,]$,则样本的主成分为$Y = V^TX$.投影矩阵为$V$

   也可以采用奇异值分解求取主成分。记$X' = \frac{1}{\sqrt{n-1}}X$,其奇异值分解为$X'=U\Sigma V^T$.则样本主成分为$Y=U_d^TX$.投影矩阵为$U$

   PCA寻找的投影方向能最大化样本方差，也即最小化样本重构误差。

7. ### 线性判别分析

   LDA寻找能最大化类间方差，最小化类内方差的投影方向。一种有监督的降维方法。

   第$i$类类内方差矩阵:$S_i = \sum_{x\in {D_i}} (x-m_i)(x-m_i)^T $,其中$D_i$第$i$类样本集合，$m_i$为$i$类样本均值。

   样本类内方差矩阵：$S_w = \sum_{i=1}^C S_i$

   样本类间方差矩阵:$S_b = \sum_{i=1}^C n_i(m_i-m)(m_i-m)^T$

   样本总体方差矩阵:$S = \sum_{i=1}^n(x_i -m)(x_i -m)^T = S_w + S_b$

   LDA目标: $\max L(W) = \frac{\mathrm {tr}(W^TS_bW)}{\mathrm {tr}(W^TS_wW)}$,且要满足$W^TW = I$

   最优解为$S_w^{-1}S_b$的最大特征值所对应的特征向量。

8. ### 流形学习 

   - 多维缩放(MDS):在已知距离矩阵的情况下，获取样本点坐标位置关系
   - 等距特征映射(ISOMAP):计算测点距离，然后调用MDS获取低维投影坐标
   - 局部线性嵌入(LLE, Locally Linear Embedding)：保持数据得局部线性性。，即每个样本点都可以用其近邻点的线性组合表示，通过保持其每个点的近邻重构系数获取低维线性嵌入。
   - 局部切空间对齐(LTSA, Local tangent Space Alignment)：该算法认为流形在局部具有线性结构，于是可以通过切空间在局部近似表达样本点（也即局部PCA坐标），然后采用alignmentg技术，将局部切空间坐标统一成低维嵌入坐标。
   - 拉普拉斯特征映射(LE, Laplacian Eigenmaps)：通过保持数据的局部相似性来实现降维，即希望将相似的样本点降维后距离较近。其原始的优化目标为 $\min \sum_{i,j} w_{ij}\|y_i-y_j\|^2$。其中$w_{ij}$表示互为近邻点的样本$x_i$和$x_j$之间的相似性，一般采用高斯相似性$w_{ij}=\exp\left(-\frac{\|x_i-x_j\|^2}{\sigma}\right)$。拉普莱斯矩阵定义为$L = D- W$，$W$为相似性矩阵，$D$为对角矩阵$d_{ii} = \sum_j w_{ij}$。则原始优化目标可以转化为$\min \mathrm {tr}(Y^TLY)$.最后可以通过矩阵特征值分解求解。

9. ###  逻辑斯蒂回归

   逻辑斯蒂回归(Logistic Regression)解决的是分类问题。令$\mu = p(y=1|x)$即样本属于正例的概率，则$1-\mu$表示样本属于 负类的概率。LR认为对数几率(log odds)或logit函数是变量$x$的线性函数（广义线性模型），即：
$$
   \mathrm{logit}(\mu) = \log \frac{\mu}{1-\mu} = w^\mathrm{T}x+b
$$
   从而:
$$
   \mu = \frac{1}{1+\exp {(-(w^\mathrm{T}x+b)})}
$$
   不是一般性，记$w=(w,b), x=(x,1)$。一般应用最大似然法来估计上述模型的参数。设$P(Y=1|x）= \pi(x), P(Y=0|x) = 1-\pi(x)$, 则训练数据的似然函数为:
$$
   \prod_{i=1}^n\pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i}
$$
   对数似然函数为：
$$
   \begin{align}L(w) &= \sum_{i=1}^n[y_i\log\pi(x_i) + (1-y_i)\log(1-\pi(x_i))] \\\\
   &= \sum_{i=1}^n[y_i\log\frac{\pi(x_i)}{1-\pi(x_i)} + \log(1-\pi(x_i))]\\\\
   &= \sum_{i=1}^n[y_i(w\cdot x_i) - \log (1+\exp(w\cdot x_i)) 
   \end{align}
$$
   对$L(w)$求极大值得到$w$的估计，可以采用梯度下降法或者拟牛顿法。

   #### 注意：

   - 最大化上述对数似然函数，等价于最小化对数损失函数：

$$
   \begin{align}
   L(w,x,y) &= -\log P(y|x) = 
   \begin{cases}
   -\log P(y=1|x) = -y\log\pi(x), &y=1\\\\
   -\log P(y=0|x)  = -(1-y)\log(1-\pi(x)), & y=0
   \end{cases}
    \\\\
    &= y\log\pi(x) + (1-y)\log(1-\pi(x))
   \end{align}
$$

   - 最大化上述对数似然函数，等价于最小化标签的真实分布和标签的条件分布之间的**KL散度**。
     $$
     KL(y,p(y|x)) = \sum_y ylogP(y|x) = y\log\pi(x) + (1-y)\log(1-\pi(x))
     $$

   逻辑斯蒂回归是二类分类模型，可将其推广到多项逻辑斯蒂回归模型。假设离散类型变量$Y$取值为集合$\{1,2,\cdots,K\}$，那么多项逻辑斯蒂回归模型的为：
$$
   \begin{align}P(Y=k|x) &= \frac{\exp(w_k\cdot x)}{1+\sum_{i=1}^{K-1}\exp(w_k\cdot x)} \\\\
   P(Y=K|x) &= \frac{1}{1+\sum_{i=1}^{K-1}\exp(w_k\cdot x)}
   \end{align}
$$
   然后通过最大化对数似然函数或者最小化对数损失函数、最小化KL散度等方法对模型参数进行估计。

11. ###  支持向量机

    两向量$u,v$夹角$\alpha$，则其内积$u\cdot v = \|u\|\|v\|\cos\alpha$

    不在同一直线的的三点$u, v, w$，点$w$到直线$\vec {uv}$的距离公式为：
    $$
    \left\|w-u - \frac{(w-u)\cdot (v-u)}{\|w-u\|\|v-u\|}\|(w-u)\|\frac{v-u}{\|v-u\|}\right\|= \|w-u - \frac{(w-u)\cdot (v-u)}{\|v-u\|^2}(v-u)\|
    $$

![点到直线的距离](.\images\distance.jpg"点到直线的距离")

支持向量机的决策函数$f(x) = sign(w\cdot x +b)$

分类超平面：$w\cdot x + b = 0$ 

点$x$到上述分类超平面的距离：$\frac{\|w\cdot x + b\|} {\|w\|}$

样本点$(x,y)$到分类超平面的**函数间隔**：$\hat \gamma = y(w\cdot x + b)$

样本点$(x,y)$到分类超平面的**几何间隔**：$\gamma =  y(w\cdot x + b)/\|w\|$

线性支持向量机的目标是在保证分类正确的前提最大化点到分类超平面的几何间隔。
$$
\begin{align}\min _{w,b}&: \frac{1}{2}\|w\|^2 \\\\
s.t&:y_i(w\cdot x_i + b) \ge 1, i=1,2,\cdots,n
\end{align}
$$
该问题是一个凸二次规划，利用拉格朗日乘子法通过对其对偶问题就行求解，拉格朗日函数：
$$
L(w,b,\alpha)= \frac{1}{2}\|w\|^2 + \sum_i\alpha_i(1-y_i(w\cdot x_i + b)) \tag{1}
$$
通过对$w, b $求导，令导数为零可得：
$$
\begin{align}w &= \sum_i \alpha_iy_ix_i \\\\
\sum_i \alpha_iy_i &=0
\end{align}
$$
将上式带入到拉格朗日函数(1)中，得到如下对偶优化问题：
$$
\begin{align}
\max_{\alpha}&: L(\alpha) = \sum_i \alpha_i - \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_iy_iy_jx_ix_j\\\\
s.t&: \sum_i \alpha_iy_i =0,  i=1,2,\cdots,n
\end{align}
$$
求出$\alpha$后，依次可求得$w, b$ ,最后得到分类函数为：
$$
f(x) = \sum_i\alpha_iy_i(x_i\cdot x) + b
$$
在上述拉格朗日求解过程中KKT条件为$(i=1,2,\cdots,n)$：
$$
\begin{align}
\alpha_i &\ge 0 \\\\
y_i(w\cdot x_i + b) &\ge 1, \\\\
\alpha_i(y_i(w\cdot x_i + b)-1) &= 0 \\\\
\end{align}
$$
上述公式中第三个等式被称为松弛条件。由上述条件可知：

- 当$\alpha_i>0$时，则$y_i(w\cdot x_i + b)=1 $，此时样本点$(x_i,y_i)$落在分类间隔的边界上，我们称其为支持向量。

- 当$\alpha_i=0$，此时所对应的的样本$(x_i,y_i)$不出现在$w$求解表达式中，对支持向量机决策边界不起任何作用。

  所以此方法被称为支持向量机，是因为决策函数仅有少数训练样本决定(支持向量)。

软间隔最大化支持向量机，求解如下优化问题($\xi$称为松弛变量)：
$$
\begin{align}
\min_{w,b,\xi}:&\frac{1}{2}\|w\|^2 + C\sum_i \xi_i\\\\
s.t:& y_i(w\cdot x_i+b) \ge 1- \xi_i, i=1,2,\cdots,n\\\\
& \xi_i \ge 0, i=1,2,\cdots,n
\end{align}
$$
其对偶优化问题为：
$$
\begin{align}\min_\alpha:& \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) -\sum_i\alpha_i\\\\
s.t:&\sum_i\alpha_iy_i=0 \\\\
&0\le\alpha_i\le C, i=1,2,\cdots,n
\end{align}
$$
软间隔支持向量机的支持向量$x_i$所对应的的$\alpha_i>0$，他们或者在间隔边界上，或者在间隔边界与分离超平面之间，或者在分离超平面误分一侧。

- 若$0<\alpha_i < C$，则$\xi_i =0$，样本$x_i$恰好落在间隔边界上；
- $\alpha_i = C$，且$0<\xi_i<1$，则样本$x_i$被正确分类，且落在间隔边界和分离超平面之间。
- $\alpha_i = C$，且$\xi_i=1$，则样本$x_i$落在分离超平面上
- $\alpha_i = C$，且$\xi_i>1$，则样本$x_i$落在分离超平面误分类一侧。

**注意**：若使用合页损失函数$L(z)= \max(0,1-z)$,则软间隔支持向量机等价于最小化正则化的合页损失函数：
$$
\min_{w,b}:\sum_i  L(1-y_i(w\cdot x_i+b))+\lambda\|w\|^2
$$
支持向量机中常使用的核函数：

- 线性核：$\kappa(x_i,x_j) = x_i^\mathrm Tx_j$
- 多项式核：$\kappa(x_i,x_j) = (x_i^\mathrm Tx_j)^d$，其中$d$为多项式的次数
- 高斯核(径向基核)：$\kappa(x_i,x_j) = \exp\left(-\frac{\|x_i - x_j\|^2}{\sigma^2} \right)$，其中$\sigma$为高斯核的带宽(width)。
- 拉普拉斯核：$\kappa(x_i,x_j) = \exp\left(-\frac{\|x_i - x_j\|}{\sigma} \right)$，其中$\sigma>0$。
- Sigmoid核：$\kappa(x_i,x_j) = \tanh (\beta x_i^\mathrm Tx_j + \alpha)$，其中$\beta>0, \alpha<0$。

12. ### 支持向量回归

    不同于支持向量机，支持向量回归希望学习一个回归模型。而一般传统的回归方法认为只有当预测值和真实值相等时，损失才为0，支持向量回归允许预测值和真实值之前存在小于$\epsilon$ 的误差，即只有当预测值和真实值误差绝对值大于$\epsilon$才计算误差。

    ![持向量回归](.\images\SVR.png"支持向量回归示意图")

    其所解决的优化问题为：
    $$
    \begin{align}
    \min_{w,b}: \frac{1}{2}\|w\|^2 + C\sum_i \cal L_\epsilon(w*x_i+b-y_i)
    \end{align}
    $$
    其中损失函数定义如下：
    $$
    \cal L_\epsilon(Z) = \left \{
    \begin{align}
    &0, &|Z|\le \epsilon\\\\
    &|Z|-\epsilon, & |Z|> \epsilon
    \end{align} 
    \right.
    $$
    引入松弛变量$\xi_i$和$\hat \xi_i$后，上述支持向量回归的最优化问题可以写成如下形式： 
    $$
    \begin{align}
    \min_{w,b}: &\frac{1}{2}\|w\|^2 + C\sum_i(\xi_i+\hat\xi_i)\\\\
    s.t:&w*x_i+b-y_i\le\epsilon + \xi_i, \\\\
    &y_i-w*x_i-b\le\epsilon + \hat\xi_i, \\\\
    &\xi_i  \ge0,\hat\xi_i\ge0, i=1,2,\cdots,n
    \end{align}
    $$
     上述问题的KKT条件为：
    $$
    \left\{\begin{align}
    \alpha_i(w*x_i+b-y_i-\epsilon-\xi_i) &= 0 \\\\
    \hat\alpha_i(y_i - w*x_i-b-\epsilon-\hat\xi_i) &= 0 \\\\
    \alpha_i\hat\alpha_i&=0 \\\\
    \xi_i\hat\xi_i&=0\\\\
    (C-\alpha_i)\xi_i&=0\\\\
    (C-\hat\alpha_i)\hat\xi_i&=0
    \end{align}
    \right.
    $$
    隔离带里边的点(不包括边界)，其满足$w*x_i+b-y_i-\epsilon-\xi_i \ne 0$ 或者$y_i - w*x_i-b-\epsilon-\hat\xi_i \ne0$。因此其对应$\alpha_i$或$\hat\alpha_i$均为0，隔离带外边(包括边界)的点满足$w*x_i+b-y_i-\epsilon-\xi_i = 0$ 或者$y_i - w*x_i-b-\epsilon-\hat\xi_i=0$。此时由KKT条件可知，其对应$\alpha_i$或$\hat\alpha_i$不为0，为支持向量。另外$\alpha_i$或$\hat\alpha_i$必有一个为0，$\xi_i% $和$\hat\xi_i$必有一个为0。

    支持向量最后的解的形式为:
    $$
    f(x) = \sum_i (\hat\alpha_i-\alpha_i)x_i^\mathrm Tx+b
    $$

13. ###  期望最大化(EM)算法

    概率模型中，有时候既有观测变量(observable variable)，又有隐变量或者潜在变量(latent variable).

    观测数据$Y=(Y_1,Y_2,\cdots,Y_n)^\mathrm T$，未观测数据$Z=(Z_1,Z_2\cdots,Z_n)^\mathrm T$。

    考虑利用最大似然估计求解模型参数:
    $$
    \max_\theta:\log P(Y|\theta) = \log \left(\sum_Z P(Y, Z|\theta)\right) = \log \left(\sum_Z P(Y|Z,\theta)P(Z|\theta)\right)
    $$
    由于上式对数中存在求和表达式，增加了求解困难。

    EM算法最后通过迭代求解，在第$i+1$步通过求解如下优化问题：
    $$
    \max_\theta: \sum_Z P(Z|Y,\theta^{(i)})\log P(Y,Z|\theta) \tag{1}
    $$
    其中$P(Z|Y,\theta^{(i)})$为用上步迭代值$\theta^{(i)}$和观测数据求出的隐变量$Z$的概率分布。

    推导：
    $$
    \begin{align}
    \log \left(\sum_Z P(Y, Z|\theta)\right) &= \log \sum_ZP(Z|Y,\theta^{(i)})\frac{P(Y,Z|\theta)}{P(Z|Y,\theta^{(i)})}   \\\\
    &\ge \sum_ZP(Z|Y,\theta^{(i)})\log \frac{P(Y,Z|\theta)}{P(Z|Y,\theta^{(i)})} \tag{Jensen不等式} 
    \end{align}
    $$
    由于$P(Z|Y,\theta^{(i)})$为常数，最大化右边等价于最大化$\sum_ZP(Z|Y,\theta^{(i)})\log P(Y,Z|\theta)$,也即公式(1).

    公式(1)又被称为**$Q$函数**，它是完全数据的对数似然函数$P(Y,Z|\theta)$关于在给定观测数据$Y$和当前参数$\theta ^{(i)}$下未观测数据$Z$的条件概率分布$P(Z|Y,\theta^{(i)})$的期望，即：

    $Q(\theta, \theta^{(i)})= \mathrm E_Z[\log P(Y,Z|\theta)|Y,\theta^{(i)}] = \sum_Z P(Z|Y,\theta^{(i)})\log P(Y,Z|\theta)  \tag{2}$

    要注意，在迭代求解公式(1)时，有时候并不需要真正求出$P(Z|Y,\theta^{(i)})$，而是在求和过程中会得到关于$P(Z|Y,\theta^{(i)})$的期望表达式，因此我们只需根据之前的参数$\theta^{(i)}$和已知数据，求出隐变量$Z$的期望。这种技巧在求解高斯混合模型、投硬币模型中经常用到。

    ![EM优化](.\images\EM.jpg"EM算法优化示意图")

    EM算法步骤：

    1. 选择参数初始值$\theta^{(0)}$,开始迭代;
    2. E步骤：按照公式(2),利用上步迭代值$\theta ^{(i)}$计算完全数据的条件期望(Q函数)。
    3. M步骤：最大化Q函数，求得$\theta ^{(i+1)}$
    4. 重复2,3步，直至手链或者最大迭代次数。

14. ### 集成学习(提升方法)

    ####  1、AdaBoost算法

    对于二分类问题，通过组合多个弱分类器（仅比随机猜测好一点），构成一个分类正确率比较高的分类器。AdaBoost算法通过提高被前一轮分类器误分类样本的权值，降低被正确分类样本的权值来改变训练数据的概率分布。设$D_m=(w_{m1},w_{m2},\cdots,w_{mN})$为第$m$个分类器训练数据的权重，初始时每个样本的权重相等，即$D_1=(w_{11},w_{12},\cdots,w_{1N}) =(\frac{1}{N},\frac{1}{N},\cdots,\frac{1}{N})$。设$G_m: \cal X \rightarrow \{-1,1\}$为第$m$个弱分类器，其分类误差率为$e_m=\sum_{i=1}^NP(G(x_i)\neq y_i)=\sum_{i=1}^Nw_{mi}\Bbb I[G(x_i)\neq y_i]$。分类器$G_m$的组合权重为：
    $$
    \alpha_m = \frac{1}{2}\ln\frac{1-e_m}{e_m}
    $$
    下一轮迭代时，其样本权重$D_{m+1}$通过如下公式计算：
    $$
    w_{m+1,i} = \frac{w_{mi}}{Z_{m+1}}\exp(-\alpha_mG(x_i)y_i)
    $$
    其中$Z_{m+1}$是规范化系数，确保$D_{m+1}$是一个概率分布。

    最终的分类器是所有弱分类器的加权线性组合。
    $$
    f(x) = sign\left(\sum_{i=1}^M\alpha_iG_i(x)\right)
    $$
    AdaBoost算法的最终训练误差界为$\prod_{i=1}^MZ_i$。另外AdaBoost算法的训练误差是以**指数速率**下降滴。这可以保证AdaBoost算法能很快收敛。

    ​	        AdaBoost算法是一种加法模型，它以指数函数为损失函数，以前向分步算法为学习算法的二分类学习方法。从偏差-方差分解的角度看，AdaBoost算法主要通过降低偏差来提高性能。

    #### 2、提升树

    ​		提升树是以CART决策树为基本分类器的提升方法。提升树模型可以表示成如下CART树的加法模型：
    $$
    f_M(x) = \sum_{m=1}^MT(x;\Theta_m)
    $$
    其中$T(x;\Theta_m)$表示CART树，$\Theta_m$为决策树的参数,$M$为决策树的个数。

    ​     	首先假设$f_0(x)=0$，则第$m$步的模型是
    $$
    f_m(x) = f_{m-1} + T(x;\Theta_m)
    $$
    其通过最小化经验风险来确定模型参数$\Theta_m$。
    $$
    \hat \Theta_m = \arg \min_{\theta_m}\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\Theta_m))
    $$
    ​		对于二分类问题，只需要基分类器限制为二分类分类树，然后采用AdaBoost算法。对于回归问题，若采用平方误差损失时，上述公式中，损失函数变为：
    $$
    \begin{align}
    \sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i;\Theta_m)) &= \sum_{i=1}^N [y_i - f_{m-1}(x_i) - T(x_i;\Theta_m)]^2 \\
    & = \sum_{i=1}^N (r_i - T(x_i;\Theta_m))^2
    \end{align}
    $$
    $r_i$为上一决策时拟合残差，因此当前决策树只需要拟合前一决策树的残差即可。

    **梯度提升树（GBDT， Gradient Boosting Decision Tree）**：当损失函数为指数损失或者平方差损失时，提升树的每一步优化比较简单，当损失函数为一般损失函数时，采用梯度提升方法算法。平方损失时每一步的优化时拟合上一步的拟合误差，而梯度提升方法则拟合负梯度在上一个模型位置上的值，即
    $$
    -\left[\frac{\partial L(x_i,f(x_i))}{\partial f(x_i)}\right]_{f_{m-1}(x_i)}
    $$

    #### 3、Bagging(Boostratp AGGregatING)

    ​        Bagging算法的基本思想是通过自助采样法(boostrap sampling)，采样出$T$个含$m$个训练样本的训练数据集，在这多个训练集上训练出多个弱分类器，然后对于分类问题，采用委员会投票得到最后的分类结果，对于回归为题，一般采用多分类器输出的平均值作为最后的输出。

    ​         自助一般采用有放回采样，因此每个分类器只使用了初始训练集中约63.2%的样本，剩余约36.8%的样本可以用作验证机来对分类器的泛化性能进行“**包外估计**”(out-of-bag estimate)。分类器包外估计的误差可以用来衡量该分类器的可信度。

    ​          从偏差-方差分解的角度看，Bagging算法主要通过降低方差来提高性能。

    #### 4、随机森林

    ​		随机森林本质上是Bagging的一个扩展变体。随机森林以决策树为基学习器，在自助采样获取训练数据集的基础上，训练过程中引入了随机属性选择。具体来说在传统决策树算法中，当前结点会从当前属性集合(假设有$d$个属性)中选择一个最优属性来进行判断扩展；而随机森林则是先从该属性集合中随机选择$k(k<d)$个属性，然后从这$k$个属性中选择最优属性进行判断扩展。

    #### 5、XGBoost

    XGBoost(Extreme Gradient Boosting Tree)可以看做是GBDT的扩展。不同于GBDT, XGBoost为了防止过拟和，在优化目标中加入限制模型复杂度的正则化项，其具体优化目标如下：
    $$
    obj = \sum_{i=1}^mL(y_i,f(x_i)) + \Omega(f(x_i))
    $$
    具体的在第$t$次迭代过程中，其优化目标为：
    $$
    obj^{(t)} = \sum_{i=1}^mL(y_i,f_{(t-1)}(x_i)+ h_t(x_i)) + \gamma J_t + \frac{1}{2}\lambda\sum_{i=1}^{J_t}w^2_{hi}
    $$
    这里$J_t$是第$t$次迭代树$h_t$的叶子节点数目，$w_{ti}$为叶子节点的取值。在构建树$h_t$过程中，需要解决两个优化问题，一个是确定最优的$J_t$个叶子节点也即划分区域，二是确定每个叶子节点的最优取值$w_{ti}$。

    不同于BGDT在优化损失函数时，模拟损失函数的负梯度，即用损失函数一阶泰勒展开近似，XGBoost对优化目标损失函数部分采用二阶泰勒展开近似，即：
    $$
    \begin{align}
    obj^{(t)} &= \sum_{i=1}^m\left(L(y_i,f_{(t-1)}(x_i)) + \frac{\partial L(y_i,f_{(t-1)}(x_i))}{\partial f_{(t-1)}(x_i)}h_t(x_i) + \frac{1}{2}\frac{\partial^2 L(y_i,f_{(t-1)}(x_i))}{\partial f^2_{(t-1)}(x_i)}h^2_t(x_i))\right) + \gamma J_t + \frac{1}{2}\lambda\sum_{i=1}^{J_t}w^2_{hi} \\
    &= \sum_{i=1}^m \left(L(y_i,f_{(t-1)}(x_i)) + g_{ti}h_t(x_i) + \frac{1}{2}h_{ti}h^2_t(x_i)\right) +  \gamma J_t + \frac{1}{2}\lambda\sum_{i=1}^{J_t}w^2_{hi} 
    \end{align}
    $$
    由于$L(y_i,f_{(t-1)}(x_i))$是常数，且每个叶子节点取值唯一，所以上述目标函数可以进一步优化：
    $$
    \begin{align}
    obj^{(t)} &\approx \sum_{i=1}^m \left(g_{ti}h_t(x_i) + \frac{1}{2}h_{ti}h^2_t(x_i)\right) +  \gamma J_t + \frac{1}{2}\lambda\sum_{i=1}^{J_t}w^2_{hi}  \\
    & = \sum_{i=1}^{J_t}\sum_{x_j\in R_{ti}}g_{tj}w_{ti} + \frac{1}{2}\sum_{i=1}^{J_t}\sum_{x_j\in R_{ti}}h_{tj}w^2_{ti}  + \gamma J_t + \frac{1}{2}\lambda\sum_{i=1}^{J_t}w^2_{hi} \\
    & = \frac{1}{2}\sum_{i=1}^{J_t}\left(\sum_{x_j\in R_{ti}}h_{tj}+\lambda\right)w^2_{ti} + \sum_{i=1}^{J_t}\sum_{x_j\in R_{ti}}g_{tj}w_{ti} + \gamma J_t \\
    &= \frac{1}{2}\sum_{i=1}^{J_t}(H_{hi}+\lambda)w^2_{hi}  + \sum_{i=1}^{J_t}G_{hi}w_{hi} + \gamma J_t
    \end{align}
    $$
    可以看出上式为关于$w_{hi}$的二次函数。所以在$h_t$树结构确定的情况下，每个叶子节点的最优取值为
    $$
    w_{hi} = -\frac{G_{hi}}{H_{hi}+\lambda}
    $$
    这就解决了树$h_t$的叶子节点的最优取值问题。这样优化目标的值为
    $$
    obj^{(t)} = -\frac{1}{2}\sum_{i=1}^{J_t}\frac{G^2_{hi}}{H_{ti}+ \lambda} + \gamma J_t
    $$
    对于树$h_t$对当前节点如何选择最优属性和最优划分值，XGBoost和GBDT一样，采用贪心策略，通过遍历所有特征和其所有划分值，寻找使$obj^{(t)}$最小的特征及其划分值。在划分节点是选择能最小化优化目标$obj^{(t)}$的属性和其划分值。

15. ###  神经网络

    神经元模型：

    ![神经元模型](.\images\神经元模型.PNG "神经元模型")

    函数$f$为非线性激活函数。

    常见激活函数：

    - 阶跃函数: $sign(x) = \left\{\begin{align} 1\qquad & x\ge 0;\\0 \qquad& x<0\end{align}\right .$

      ![阶跃函数图像](D:\jack_doc\python_src\ml_notes.git\images\sign function.PNG "阶跃函数图像")

      

    - sigmoid函数：$f(x)=\frac{1}{1+\exp{(-x)}}$

      ![sigmoid函数](.\images\sigmoid function.PNG "sigmoid函数")

      sigmoid函数导数：
      $$
      f'(x) = f(x)(1-f(x))
      $$
      从函数图像可以看出，当$x$特别小或者特别大时，sigmoid导数趋向于0，这会导致在神经网络中方向传播过程中，梯度更新特别缓慢，出现梯度消失现象。

    - tanh函数：
      $$
      f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
      $$
      ![tanh函数](.\images\tanh函数.jpg "tanh函数")

      tanh函数导数：$f'(x) = 1-f(x)^2$

      tanh函数同sigmoid函数的一样，当$x$特别大或者特别小时，导数趋向于0。

    - relu函数:ReLU函数又称为**修正线性单元（Rectified Linear Unit）**，是一种分段线性函数，其弥补了sigmoid函数以及tanh函数的**梯度消失问题**。
      $$
      f(x) = \left\{\begin{align}x\qquad &x\ge 0 \\ 0 \qquad &x<0\end{align}\right.
      $$
      ![relu function](D:\jack_doc\python_src\ml_notes.git\images\relu function.jpg)

      relu函数导数：
      $$
      f'(x) = \left\{\begin{align}1\qquad &x\ge 0 \\ 0 \qquad &x<0\end{align}\right.
      $$
      

    - Leaky relu函数: 对relu函数的一种改进。relu函数在$x$值为负时导数为0 ，也存在梯度消失问题。
      $$
      f(x) = \left\{\begin{align}x\qquad &x\ge 0 \\ \alpha x \qquad &x<0\end{align}\right.
      $$
      其中$0<\alpha<1$.

      ![](D:\jack_doc\python_src\ml_notes.git\images\leaky relu.jpg)

      Leaky ReLU函数的**导数**为：
      $$
      f'(x) = \left\{\begin{align}1\qquad &x\ge 0 \\ \alpha \qquad &x<0\end{align}\right.
      $$

    多层神经网络若全部采用线性激活函数，则无论层数如何增加，最终的输出都是输入的线性组合，因此达不到非线性拟合的要求。通过采用非线性激活函数，增加网络层数，可以实现拟合各种有趣和复杂的函数。

16. ###  最大熵模型

    最大熵原理认为，在所有满足约束条件的概率分布中，熵最大的模型是最优的。符合人们直观认识，即在满足已知事实情况下，我们不能再对不确定部分做任何假设，即不确定性部分应该是“等可能”出现的(熵最大化)。

    在最大熵模型中，一般用特征函数(feature function)来描述已知约束条件。其定义为：
    $$
    f(x,y) = \left\{\begin{align}1, & \quad x,y满足某一条件,
    \\
    0, & \quad 否则.
    \end{align}\right.
    $$
    特征函数关于经验分布的期望$E_\tilde p(f) = \sum_{x,y} \tilde p(x,y)f(x,y)$,$\tilde p(x,y)$为训练数据的经验分布。

    特征函数$f(x,y)$关于模型$p(y/x)$与经验分布$\tilde p(x)$的期望为$E_p(f) = \sum_{x,y} \tilde p(x)p(y|x)f(x,y)$.如果能够获取训练数据中的嘻嘻，那么就可以假设这两个期望值相等，即$E_p(f) = E_\tilde p (f)$。所以最大熵模型问题为：

    > 假设模型所满足的约束条件集合为$C= \{ p\in P| E_p(f_i) = E_\tilde p(f_i),i = 1,2,\dots,n\}$,定义在条件概率$p(y|x)$上的条件熵为:$H(p) = -\sum_{x,y}\tilde p(x)p(y|x)\log p(y|x)$.则模型集合$C$中条件熵$H(p)$最大的模型为最大熵模型。

17. ###  隐马尔可夫模型

    三要素：初始状态概率向量$\pi$、状态转移概率矩阵$A$、观测概率矩阵$B$. $\lambda = (\pi,A,B)$

    齐次马尔科夫假设：隐藏的马尔科夫链在任意时刻$t$的状态只依赖于其上一时刻的状态，与其他时刻状态无关。$p(i_t|i_{t-1},\cdots,i_1) = p(i_t|i_{t-1})$.

    观测独立性假设：任意时刻观测值只依赖于此时刻的状态，与其他时刻状态无关。$p(o_t|i_t,o_{t-1},i_{t-1},\cdots,o_1,i_1) = p(o_t|i_t)$.

    观测序列的生成过程：

    > 1、按初始分布$\pi$ 产生初始状态$i_1$
    >
    > 2、对任意时刻$t=1,2,\cdots,T$
    >
    > 3、按照状态$i_t$的观测概率分布B_{i_t}(k)生成观测$o_t$_
    >
    > 4、按照状态$i_t$的状态转移概率分布$a_{i_ti_{t+1}}$产生状态$i_{t+1}$

    隐马尔科夫三个基本问题：

    1、**概率计算问题：**给定模型$\lambda = (\pi,A,B)$下计算观测序列$O=(o_1,o_2,\cdots,o_T)$的概率$p(O|\lambda)$.采用前向后向计算方法。

    2、**学习问题：**已知观测序列$O=(o_1,o_2,\cdots,o_T)$，估计模型参数$\lambda = (\pi,A,B)$,使得在该模型下，观测序列$p(O|\lambda)$最大，使用极大似然估计即可求解。采用Baum-Welch算法，利用EM算法求解

    3、预测问题：也称解码问题。给定模型$\lambda = (\pi,A,B)$和观测序列$O=(o_1,o_2,\cdots,o_T)$，求在该观测序列条件下最有可能的状态序列，即求使$p(I|O)$最大的状态序列。维特比算法，该算法采用动态规划算法求解。

18. ### 条件随机场











