# 自然语言处理算法

1. ### n-gram模型

   # [N-gram 语言模型](https://sunoonlee.github.io/2017/03/ngram/)

   **二、N-Gram模型详解**

   既然要做语言模型，基于统计概率来说，我们需要计算句子的概率大小：

   ![img](https://mmbiz.qpic.cn/mmbiz_png/lFAia5dnhjrWhGNWDkdibGKCN0rDMeenibLf3ibr3XPLyKt9MC6oZ9otUzLMNTAGzsnq5ggiaibUM2NuIIIWyfRTAtVA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

    ，这个也就是最终要求的一句话的概率了，概率大，说明更合理，概率小，说明不合理，不是人话。。。。

   因为是不能直接计算，所以我们先应用条件概率得到

   ![img](https://mmbiz.qpic.cn/mmbiz_png/lFAia5dnhjrWhGNWDkdibGKCN0rDMeenibLzMgD46GYojV8WyqV5vALf4orbiblWuL8GTI4sq13cpTicmpibK0xZBJLA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

   中间插入下条件概率： P(B|A)：A 条件下 B 发生的概率。从一个大的空间进入到一个子空间（切片），计算在子空间中的占比。

   ![img](https://mmbiz.qpic.cn/mmbiz_jpg/lFAia5dnhjrWhGNWDkdibGKCN0rDMeenibL9hOia12zEib7mYIZs8ib1xxKkmy0Trkx5xNc8ib4VXzDibdWI8ScU3gWDsQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

   然而，如果直接算条件概率转化后的式子的话，对每个词要考虑它前面的所有词，这在实际中意义不大，显然并不好算。那这个时候我们可以添加什么假设来简化吗？可以的，我们可以基于马尔科夫假设来做简化。

   **什么是马尔科夫假设？**

   马尔科夫假设是指，每个词出现的概率只跟它前面的少数几个词有关。比如，二阶马尔科夫假设只考虑前面两个词，相应的语言模型是三元模型。引入了马尔科夫假设的语言模型，也可以叫做马尔科夫模型。

   马尔可夫链（Markov chain）为状态空间中经过从一个状态到另一个状态的转换的随机过程。该过程要求具备“无记忆”的性质：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。

   也就是说，应用了这个假设表明了当前这个词仅仅跟前面几个有限的词相关，因此也就不必追溯到最开始的那个词，这样便可以大幅缩减上述算式的长度。即式子变成了这样：

   ![img](https://mmbiz.qpic.cn/mmbiz_png/lFAia5dnhjrWhGNWDkdibGKCN0rDMeenibLcM2hA8wMcNsxpRlkibzDJRDjVwgCv8b4CJg5eeK8OkEzY4WPnOCOQibA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

   > 注：这里的m表示前m个词相关

   然后，我们就可以设置m=1，2，3，....得到相应的一元模型，二元模型，三元模型了，关于

   当 m=1, 一个一元模型（unigram model)即为 ：

   ![img](https://mmbiz.qpic.cn/mmbiz_jpg/lFAia5dnhjrWhGNWDkdibGKCN0rDMeenibL7pm4189eDWibBbN88hY4FBohd5d3gCupVsiczuvA5V3VZUkObNEDTpnA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

   当 m=2, 一个二元模型（bigram model)即为 ：

   ![img](https://mmbiz.qpic.cn/mmbiz_jpg/lFAia5dnhjrWhGNWDkdibGKCN0rDMeenibLTzQCjTvTU203MVtXJHEwiaAorETG2icGetX3joDkrhMOibUgSTb4F5ERA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

   当 m=3, 一个三元模型（trigram model)即为

   ![img](https://mmbiz.qpic.cn/mmbiz_jpg/lFAia5dnhjrWhGNWDkdibGKCN0rDMeenibL6o4WWowrg176ggSkBsicq2vZenOrAgtSiaDLP1onSQmCR8rdOuicY9lIQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

   而N-Gram模型也就是这样，当m=1，叫1-gram或者unigram ；m=2，叫2-gram或者bigram ；当 m=3叫3-gram或者trigram ；当m=N时，就表示的是N-gram啦。

2. ### 神经网络语言模型

   ![NNLM](.\images\NNLM.jpg)

3. ### 词向量

   **word2vec**  《Efficient Estimation of Word Representation in Vector Space》

   github开源地址：

   1. gensim(Topic Modelling in Python) :    https://github.com/RaRe-Technologies/gensim/
   2. Python interface to Google word2vec: https://github.com/danielfrg/word2vec

   

   ![word2vec](.\images\word2vec.jpg)

   

   **golve **  《Glove: Global vectors for word representation》

   word2vec只考虑到了词的局部信息，没有考虑到词与局部窗口外词的联系，glove利用共现矩阵，同时考虑了局部信息和整体的信息。

   

   **ELMO**  《Deep contextualized word representations》

   github地址：https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md

   word2vec和glove由于用固定词向量表示词，因此无法解决一词多意现象。Elmo针对不同语境(上下文)，给出词的不同向量表示。

   ![ELMO](.\images\ELMO.jpg)

   **bert**  《Bidirectional Encoder Representation from Transformers》

   github地址:   https://github.com/google-research/bert

4. ### seq2seq

   基本的 Encoder-Decoder模型 ,编码器和解码器都是RNN模型，

   一种是解码器将原始句子编码为固定向量$c$，然后将$c$作为初始状态提供给加码器，如下图。

   ![](.\images\编码解码模型.png)

   另一种是将编码器输出的状态向量$c$当做加码器RNN每一步的输入。

   由于![](.\images\编码解码模型2.png)

   这种Encoder-Decoder结构不限制输入和输出的序列长度，因此应用的范围非常广泛，比如：

   **机器翻译。**Encoder-Decoder的最经典应用，事实上这一结构就是在机器翻译领域最先提出的

   **文本摘要**。输入是一段文本序列，输出是这段文本序列的摘要序列。

   **阅读理解。**将输入的文章和问题分别编码，再对其进行解码得到问题的答案。

   **语音识别。**输入是语音信号序列，输出是文字序列。

   

5. ### 注意力机制

6. ### transformer模型

   《all atttention is you need》

   

7. GPT模型

   文章《Improving Language Understanding by Generative Pre-Training》

   github地址: https://github.com/openai/finetune-transformer-lm

   

   OpenAI提出的Generative Pre-Training，采用transformer作为基本特征提取单元(仅使用解码器部分)，pre-training+Fine-Tuning的训练模式。单向二阶段训练模型。

   单向语言模型(句子中词只能看到他前面的词)，将transformer编码器模块的多头自注意力木块改成Masked-self attention。

   ![](.\images\GPT_1.jpg)

   ![](.\images\GPT_2.jpg)

   1. 预训练阶段，采用无监督数据，用前面的词预测后面的词，最后一位的输出不用。

   2. 精调阶段，采用监督数据，用最后一位的输出做分类。为了避免过拟合，除了用最后一位进行预测，前面的词还和预训练阶段一样，预测下一个词。所以损失函数由两部分组成:
      $$
      \begin{align}
      L_1(\cal C) &= \sum_i \log P(u_i|u_{i-k},\cdots,u_{i-1};\Theta)\\
      L_2(\cal C) &= \sum_{(x,y)}\log P(y|x^1,x^2,\cdots,x^m) \\ 
      L_3(\cal C) &= L_1(\cal C) + \lambda L_2(\cal C)  \\
      \end{align}
      $$
      针对不同任务，输入做不同改造：

      ![](.\images\GPT_3.jpg)

8. ### bert模型

   github地址：https://github.com/google-research/bert

    

9. MT-DNN

   微软多任务深度神经网络，对bert 进行多任务训练。MT-DNN扩展了微软在2015年提出的**多任务DNN模型(Multi-Task DNN)**，引入了谷歌AI开发的预训练双向transformer语言模型BERT。

   《Multi-Task Deep Neural Networks for Natural Language Understanding》

   github地址：https://github.com/namisan/mt-dnn

   ![](.\images\MT-DNN.png)

10. GPT 2.0 

    《Language Models are Unsupervised Multitask Learners》

    github地址：https://github.com/openai/gpt-2

11. MASS

    《MASS: Masked Sequence to Sequence Pre-training for Language Generation》

    github地址： https://github.com/microsoft/MASS

12. XLNet

13. 

14. ulm